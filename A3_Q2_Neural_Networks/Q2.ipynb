{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = \"/home/tkarthikeyan/IIT Delhi/COL774-Machine Learning/Assignment 3/Data_b/x_train.npy\"\n",
    "y_train_path = \"/home/tkarthikeyan/IIT Delhi/COL774-Machine Learning/Assignment 3/Data_b/y_train.npy\"\n",
    "\n",
    "X_train, y_train = get_data(x_train_path, y_train_path)\n",
    "\n",
    "x_test_path = \"/home/tkarthikeyan/IIT Delhi/COL774-Machine Learning/Assignment 3/Data_b/x_test.npy\"\n",
    "y_test_path = \"/home/tkarthikeyan/IIT Delhi/COL774-Machine Learning/Assignment 3/Data_b/y_test.npy\"\n",
    "\n",
    "X_test, y_test = get_data(x_test_path, y_test_path)\n",
    "\n",
    "#you might need one hot encoded y in part a,b,c,d,e\n",
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n, n_hidden_nodes, r, M):\n",
    "        #Number of nodes in the architecture\n",
    "        self.n = n\n",
    "        self.n_hidden_nodes = n_hidden_nodes\n",
    "        self.r = r\n",
    "        \n",
    "        #Mini batch size\n",
    "        self.M = M\n",
    "        \n",
    "        #Weights and biases\n",
    "        self.W = dict()\n",
    "        self.b = dict()\n",
    "        \n",
    "    def initialize_weights_and_biases(self):\n",
    "        n_nodes = [self.n] + self.n_hidden_nodes + [self.r]\n",
    "        \n",
    "        #Initialize weights\n",
    "        for i in range(1,len(n_nodes)):\n",
    "            self.W[str(i)] = np.random.uniform(low=-0.1, high=0.1, size=(n_nodes[i], n_nodes[i-1]))\n",
    "        \n",
    "        #Initialize biases\n",
    "        for i in range(1,len(n_nodes)):\n",
    "            self.b[str(i)] = np.zeros((n_nodes[i],1))\n",
    "     \n",
    "    @staticmethod\n",
    "    def sigmoid(x, derivative = False):\n",
    "        if derivative == False:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            return NeuralNetwork.sigmoid(x, derivative = False) * (1 - NeuralNetwork.sigmoid(x, derivative = False))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x, derivative = False):\n",
    "        if derivative == True:\n",
    "            return np.where(x > 0, 1, np.where(x < 0, 0, np.random.random_sample()))\n",
    "        else:\n",
    "            return np.where(x <= 0, 0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "    def train(self, X_train, y_train, epoch_mode = True, activation=\"sigmoid\", EPOCHS = 200, alpha = 0.01, stopping_threshold = None, adaptive_learning=False, printafter=20):\n",
    "        self.initialize_weights_and_biases()\n",
    "        \n",
    "        a = dict()\n",
    "        z = dict()\n",
    "        del_z = dict()\n",
    "        del_b = dict()\n",
    "        del_W = dict()\n",
    "        \n",
    "        if epoch_mode == True:\n",
    "            for epoch in range(EPOCHS):\n",
    "                for i in range(0, X_train.shape[0], self.M):\n",
    "                    y_actual = y_train[i:i+self.M,:].T\n",
    "                    \n",
    "                    #Forward\n",
    "                    a[\"0\"] = X_train[i:i+self.M,:].T\n",
    "                    \n",
    "                    for j in range(1,len(self.n_hidden_nodes)+1):\n",
    "                        z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "                        if activation == \"relu\":\n",
    "                            a[str(j)] = NeuralNetwork.relu(z[str(j)])\n",
    "                        else:\n",
    "                            a[str(j)] = NeuralNetwork.sigmoid(z[str(j)])\n",
    "                    \n",
    "                    j += 1\n",
    "                    z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "                    a[str(j)] = NeuralNetwork.softmax(z[str(j)])\n",
    "                    \n",
    "                    #Backward\n",
    "                    del_z[str(j)] = a[str(j)] - y_actual\n",
    "                    del_b[str(j)] = np.sum(del_z[str(j)], axis = 1).reshape(-1,1)\n",
    "                    del_W[str(j)] = np.matmul(del_z[str(j)], a[str(j-1)].T)\n",
    "                    \n",
    "                    for k in range(j-1,0,-1):\n",
    "                        if activation == \"relu\":\n",
    "                            del_z[str(k)] = np.matmul(self.W[str(k+1)].T, del_z[str(k+1)])*(NeuralNetwork.relu(z[str(k)], derivative=True))\n",
    "                        else:\n",
    "                            del_z[str(k)] = np.matmul(self.W[str(k+1)].T, del_z[str(k+1)])*(NeuralNetwork.sigmoid(z[str(k)], derivative=True))\n",
    "                        del_b[str(k)] = np.sum(del_z[str(k)], axis = 1).reshape(-1,1)\n",
    "                        del_W[str(k)] = np.matmul(del_z[str(k)], a[str(k-1)].T)\n",
    "                    \n",
    "                    #Update\n",
    "                    for l in range(1,len(self.n_hidden_nodes)+2):\n",
    "                        if adaptive_learning == False:\n",
    "                            self.W[str(l)] = self.W[str(l)] - alpha * del_W[str(l)]\n",
    "                            self.b[str(l)] = self.b[str(l)] - alpha * del_b[str(l)]\n",
    "                        else:\n",
    "                            self.W[str(l)] = self.W[str(l)] - (alpha/np.sqrt(epoch)) * del_W[str(l)]\n",
    "                            self.b[str(l)] = self.b[str(l)] - (alpha/np.sqrt(epoch)) * del_b[str(l)]\n",
    "                        \n",
    "                y_pred, softmax_output = NN.predict(X_train, activation=activation)\n",
    "                softmax_loss = NeuralNetwork.compute_softmax_loss(softmax_output, y_train_onehot)\n",
    "                if epoch%printafter==0:\n",
    "                    print(f\"epoch {epoch}\")\n",
    "                    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred))\n",
    "                    if adaptive_learning:\n",
    "                        print(\"learning rate: \",(alpha/np.sqrt(epoch)))\n",
    "                    print(\"softmax loss: \",softmax_loss)\n",
    "                    print(\"\\n\")\n",
    "\n",
    "        else:\n",
    "            epoch = 1\n",
    "            window = 5\n",
    "            loss_avg = 0\n",
    "            while(True):\n",
    "                for i in range(0, X_train.shape[0], self.M):\n",
    "                    y_actual = y_train[i:i+self.M,:].T\n",
    "                    \n",
    "                    #Forward\n",
    "                    a[\"0\"] = X_train[i:i+self.M,:].T\n",
    "                    \n",
    "                    for j in range(1,len(self.n_hidden_nodes)+1):\n",
    "                        z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "                        if activation == \"relu\":\n",
    "                            a[str(j)] = NeuralNetwork.relu(z[str(j)])\n",
    "                        else:\n",
    "                            a[str(j)] = NeuralNetwork.sigmoid(z[str(j)])\n",
    "                    \n",
    "                    j += 1\n",
    "                    z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "                    a[str(j)] = NeuralNetwork.softmax(z[str(j)])\n",
    "                    \n",
    "                    #Backward\n",
    "                    del_z[str(j)] = a[str(j)] - y_actual\n",
    "                    del_b[str(j)] = np.sum(del_z[str(j)], axis = 1).reshape(-1,1)\n",
    "                    del_W[str(j)] = np.matmul(del_z[str(j)], a[str(j-1)].T)\n",
    "                    \n",
    "                    for k in range(j-1,0,-1):\n",
    "                        if activation == \"relu\":\n",
    "                            del_z[str(k)] = np.matmul(self.W[str(k+1)].T, del_z[str(k+1)])*(NeuralNetwork.relu(z[str(k)], derivative=True))\n",
    "                        else:\n",
    "                            del_z[str(k)] = np.matmul(self.W[str(k+1)].T, del_z[str(k+1)])*(NeuralNetwork.sigmoid(z[str(k)], derivative=True))\n",
    "                        del_b[str(k)] = np.sum(del_z[str(k)], axis = 1).reshape(-1,1)\n",
    "                        del_W[str(k)] = np.matmul(del_z[str(k)], a[str(k-1)].T)\n",
    "                    \n",
    "                    #Update\n",
    "                    for l in range(1,len(self.n_hidden_nodes)+2):\n",
    "                        if adaptive_learning == False:\n",
    "                            self.W[str(l)] = self.W[str(l)] - alpha * del_W[str(l)]\n",
    "                            self.b[str(l)] = self.b[str(l)] - alpha * del_b[str(l)]\n",
    "                        else:\n",
    "                            self.W[str(l)] = self.W[str(l)] - (alpha/np.sqrt(epoch)) * del_W[str(l)]\n",
    "                            self.b[str(l)] = self.b[str(l)] - (alpha/np.sqrt(epoch)) * del_b[str(l)]\n",
    "                        \n",
    "                y_pred, softmax_output = NN.predict(X_train, activation=activation)\n",
    "                softmax_loss = NeuralNetwork.compute_softmax_loss(softmax_output, y_train_onehot)\n",
    "                if epoch%printafter==0:\n",
    "                    print(f\"epoch {epoch}\")\n",
    "                    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred))\n",
    "                    if adaptive_learning:\n",
    "                        print(\"learning rate: \",(alpha/np.sqrt(epoch)))\n",
    "                    print(\"softmax loss: \",softmax_loss)\n",
    "                    print(\"\\n\")\n",
    "                \n",
    "                if epoch <= window:\n",
    "                    loss_avg += softmax_loss\n",
    "                    if epoch == window:\n",
    "                        loss_avg /= window\n",
    "                \n",
    "                #End the training\n",
    "                if epoch > window:\n",
    "                    new_loss_avg = ((window - 1)*loss_avg + softmax_loss)/window\n",
    "                    diff_avg_loss = abs(new_loss_avg - loss_avg)\n",
    "                    #print(\"diff avg loss:\",diff_avg_loss)\n",
    "                    if diff_avg_loss < stopping_threshold or epoch > EPOCHS:\n",
    "                        print(\"Convergence criteria satisfied!\")\n",
    "                        print(f\"epoch {epoch}\")\n",
    "                        print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred))\n",
    "                        if adaptive_learning:\n",
    "                            print(\"learning rate: \",(alpha/np.sqrt(epoch)))\n",
    "                        print(\"softmax loss: \",softmax_loss)\n",
    "                        print(\"\\n\")\n",
    "                        break\n",
    "                \n",
    "                epoch += 1\n",
    "                        \n",
    "    def predict(self, X_test, activation=\"sigmoid\"):\n",
    "        y_pred = np.zeros((X_test.shape[0], self.r))\n",
    "        softmax_output = np.zeros((X_test.shape[0], self.r))\n",
    "        z = dict()\n",
    "        a = dict()\n",
    "        \n",
    "        for i in range(X_test.shape[0]):\n",
    "            a[str(0)] = X_test[i:i+1,:].T\n",
    "            \n",
    "            for j in range(1,len(self.n_hidden_nodes)+1):\n",
    "                z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "                if activation == \"relu\":\n",
    "                    a[str(j)] = NeuralNetwork.relu(z[str(j)])\n",
    "                else:\n",
    "                    a[str(j)] = NeuralNetwork.sigmoid(z[str(j)])\n",
    "            \n",
    "            j += 1\n",
    "            z[str(j)] = np.matmul(self.W[str(j)], a[str(j-1)]) + self.b[str(j)]\n",
    "            a[str(j)] = NeuralNetwork.softmax(z[str(j)])\n",
    "            \n",
    "            softmax_output[i] = a[str(j)].flatten()\n",
    "            y_pred[i][np.argmax(a[str(j)])] = 1\n",
    "            \n",
    "        return y_pred, softmax_output\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_softmax_loss(softmax_output, y_pred):\n",
    "        #Softmax loss\n",
    "        sm_loss = 0\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            sm_loss = -1*np.log2(softmax_output[i][np.argmax(y_pred[i])])\n",
    "            \n",
    "        return sm_loss/(y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.00023525478839202835\n",
      "\n",
      "\n",
      "epoch 10\n",
      "accuracy on train data:  0.5923\n",
      "softmax loss:  0.00011264389363753989\n",
      "\n",
      "\n",
      "epoch 20\n",
      "accuracy on train data:  0.6572\n",
      "softmax loss:  5.831073565883006e-05\n",
      "\n",
      "\n",
      "epoch 30\n",
      "accuracy on train data:  0.6702\n",
      "softmax loss:  4.656056662492289e-05\n",
      "\n",
      "\n",
      "epoch 40\n",
      "accuracy on train data:  0.6758\n",
      "softmax loss:  3.9645451044889796e-05\n",
      "\n",
      "\n",
      "epoch 50\n",
      "accuracy on train data:  0.6803\n",
      "softmax loss:  3.4488996828750714e-05\n",
      "\n",
      "\n",
      "epoch 60\n",
      "accuracy on train data:  0.6869\n",
      "softmax loss:  3.0593861550354343e-05\n",
      "\n",
      "\n",
      "epoch 70\n",
      "accuracy on train data:  0.6908\n",
      "softmax loss:  2.8066258449785907e-05\n",
      "\n",
      "\n",
      "epoch 80\n",
      "accuracy on train data:  0.6976\n",
      "softmax loss:  2.7373041902283903e-05\n",
      "\n",
      "\n",
      "epoch 90\n",
      "accuracy on train data:  0.7037\n",
      "softmax loss:  2.8412086233645623e-05\n",
      "\n",
      "\n",
      "accuracy on test data:  0.714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       224\n",
      "           1       0.75      0.76      0.76       196\n",
      "           2       0.56      0.64      0.60       176\n",
      "           3       0.49      0.52      0.51       177\n",
      "           4       0.80      0.66      0.72       227\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1000\n",
      "   macro avg       0.71      0.70      0.70      1000\n",
      "weighted avg       0.72      0.71      0.72      1000\n",
      " samples avg       0.71      0.71      0.71      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork(n = 1024, n_hidden_nodes = [100,50] , r = 5, M = 32)\n",
    "NN.train(X_train, y_train_onehot, activation=\"sigmoid\", EPOCHS=100, alpha=0.001, printafter=10)\n",
    "y_pred, softmax_output = NN.predict(X_test, activation=\"sigmoid\")\n",
    "print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred))\n",
    "get_metric(y_test_onehot, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.7029924239467376\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score: \",f1_score(y_test_onehot, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [1]\n",
      "epoch 50\n",
      "accuracy on train data:  0.4706\n",
      "softmax loss:  0.00014958531153011078\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.00023287816358262217\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.0002328781641601466\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.00023287816494873916\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.00023287816496746151\n",
      "\n",
      "\n",
      "accuracy on train data:  0.2091\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "   micro avg       0.21      0.21      0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      " samples avg       0.21      0.21      0.21     10000\n",
      "\n",
      "accuracy on test data:  0.187\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "   micro avg       0.19      0.19      0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      " samples avg       0.19      0.19      0.19      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50\n",
      "accuracy on train data:  0.6775\n",
      "softmax loss:  5.1494534443521053e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.6858\n",
      "softmax loss:  3.220427274554847e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.6952\n",
      "softmax loss:  2.7774495226855474e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.6982\n",
      "softmax loss:  2.603425353508207e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.6977\n",
      "softmax loss:  2.6049241749681784e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.6977\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      2176\n",
      "           1       0.64      0.77      0.70      1652\n",
      "           2       0.73      0.56      0.63      2544\n",
      "           3       0.41      0.56      0.47      1447\n",
      "           4       0.77      0.73      0.75      2181\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     10000\n",
      "   macro avg       0.70      0.70      0.69     10000\n",
      "weighted avg       0.72      0.70      0.70     10000\n",
      " samples avg       0.70      0.70      0.70     10000\n",
      "\n",
      "accuracy on test data:  0.689\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91       246\n",
      "           1       0.63      0.76      0.69       163\n",
      "           2       0.72      0.58      0.64       248\n",
      "           3       0.41      0.49      0.45       158\n",
      "           4       0.69      0.70      0.69       185\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1000\n",
      "   macro avg       0.68      0.68      0.68      1000\n",
      "weighted avg       0.71      0.69      0.69      1000\n",
      " samples avg       0.69      0.69      0.69      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [10]\n",
      "epoch 50\n",
      "accuracy on train data:  0.6885\n",
      "softmax loss:  2.3303944233466556e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7189\n",
      "softmax loss:  2.3564423526932722e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7241\n",
      "softmax loss:  2.151708934034692e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7438\n",
      "softmax loss:  1.7535741967859166e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.7436\n",
      "softmax loss:  1.7259669594328303e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.7436\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1930\n",
      "           1       0.75      0.81      0.78      1852\n",
      "           2       0.67      0.66      0.66      2000\n",
      "           3       0.51      0.59      0.55      1752\n",
      "           4       0.85      0.72      0.78      2466\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     10000\n",
      "   macro avg       0.74      0.74      0.74     10000\n",
      "weighted avg       0.75      0.74      0.75     10000\n",
      " samples avg       0.74      0.74      0.74     10000\n",
      "\n",
      "accuracy on test data:  0.724\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       212\n",
      "           1       0.73      0.76      0.75       191\n",
      "           2       0.65      0.64      0.65       204\n",
      "           3       0.51      0.55      0.53       175\n",
      "           4       0.80      0.69      0.74       218\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      1000\n",
      "   macro avg       0.72      0.72      0.72      1000\n",
      "weighted avg       0.73      0.72      0.72      1000\n",
      " samples avg       0.72      0.72      0.72      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [50]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7392\n",
      "softmax loss:  2.6987471937410535e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7839\n",
      "softmax loss:  1.7227472892700306e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7706\n",
      "softmax loss:  1.6560629492585795e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7877\n",
      "softmax loss:  8.237749203355322e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.788\n",
      "softmax loss:  7.399416514018198e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.788\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1878\n",
      "           1       0.79      0.88      0.84      1783\n",
      "           2       0.64      0.72      0.68      1734\n",
      "           3       0.65      0.62      0.64      2106\n",
      "           4       0.91      0.76      0.83      2499\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.79      0.79      0.79     10000\n",
      "weighted avg       0.79      0.79      0.79     10000\n",
      " samples avg       0.79      0.79      0.79     10000\n",
      "\n",
      "accuracy on test data:  0.735\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       215\n",
      "           1       0.76      0.82      0.79       183\n",
      "           2       0.55      0.65      0.60       168\n",
      "           3       0.63      0.52      0.57       229\n",
      "           4       0.76      0.70      0.73       205\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1000\n",
      "   macro avg       0.73      0.74      0.73      1000\n",
      "weighted avg       0.73      0.73      0.73      1000\n",
      " samples avg       0.73      0.73      0.73      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [100]\n",
      "epoch 50\n",
      "accuracy on train data:  0.733\n",
      "softmax loss:  4.538836493085465e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7818\n",
      "softmax loss:  1.8238639023683118e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8035\n",
      "softmax loss:  9.2365944713612e-06\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8139\n",
      "softmax loss:  9.666839084236848e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8133\n",
      "softmax loss:  9.72563184360032e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8133\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      2010\n",
      "           1       0.82      0.92      0.87      1766\n",
      "           2       0.74      0.77      0.75      1883\n",
      "           3       0.64      0.67      0.65      1914\n",
      "           4       0.88      0.76      0.82      2427\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     10000\n",
      "   macro avg       0.81      0.82      0.81     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      " samples avg       0.81      0.81      0.81     10000\n",
      "\n",
      "accuracy on test data:  0.77\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       228\n",
      "           1       0.74      0.84      0.79       174\n",
      "           2       0.65      0.68      0.67       191\n",
      "           3       0.63      0.59      0.61       197\n",
      "           4       0.82      0.73      0.78       210\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1000\n",
      "   macro avg       0.76      0.77      0.76      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      " samples avg       0.77      0.77      0.77      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[1],[5],[10],[50],[100]]\n",
    "number_of_hidden_units = [1,5,10,50,100]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    NN = NeuralNetwork(n = 1024, n_hidden_nodes = hidden_layer , r = 5, M = 32)\n",
    "    NN.train(X_train, y_train_onehot, epoch_mode= False, activation=\"sigmoid\", alpha = 0.01, stopping_threshold = 1.0e-06, printafter=50)\n",
    "    y_pred_train, _ = NN.predict(X_train)\n",
    "    y_pred_test, _ = NN.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different number of hidden units\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(number_of_hidden_units, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(number_of_hidden_units, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs number of hidden units')\n",
    "plt.xlabel('number of hidden units')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(number_of_hidden_units)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [512]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7327\n",
      "softmax loss:  3.9801766116042114e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7921\n",
      "softmax loss:  5.780218764128989e-06\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8118\n",
      "softmax loss:  3.7589816123345803e-06\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8299\n",
      "softmax loss:  1.2992431023633802e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.831\n",
      "softmax loss:  1.239104246690159e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.831\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1933\n",
      "           1       0.87      0.92      0.89      1863\n",
      "           2       0.70      0.82      0.75      1669\n",
      "           3       0.75      0.66      0.70      2286\n",
      "           4       0.87      0.81      0.84      2249\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10000\n",
      "   macro avg       0.83      0.84      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      " samples avg       0.83      0.83      0.83     10000\n",
      "\n",
      "accuracy on test data:  0.782\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       223\n",
      "           1       0.81      0.88      0.84       181\n",
      "           2       0.58      0.72      0.64       162\n",
      "           3       0.73      0.56      0.64       241\n",
      "           4       0.80      0.77      0.78       193\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1000\n",
      "   macro avg       0.78      0.79      0.78      1000\n",
      "weighted avg       0.78      0.78      0.78      1000\n",
      " samples avg       0.78      0.78      0.78      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7299\n",
      "softmax loss:  2.5517614437811225e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7836\n",
      "softmax loss:  6.656917643055116e-06\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8126\n",
      "softmax loss:  5.07376437203254e-06\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8213\n",
      "softmax loss:  6.25927649641769e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8206\n",
      "softmax loss:  6.134507568326737e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8206\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1943\n",
      "           1       0.91      0.90      0.90      1997\n",
      "           2       0.71      0.83      0.76      1661\n",
      "           3       0.58      0.68      0.63      1733\n",
      "           4       0.93      0.73      0.82      2666\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.84      0.82      0.82     10000\n",
      " samples avg       0.82      0.82      0.82     10000\n",
      "\n",
      "accuracy on test data:  0.78\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       226\n",
      "           1       0.88      0.84      0.86       206\n",
      "           2       0.59      0.76      0.66       154\n",
      "           3       0.57      0.57      0.57       186\n",
      "           4       0.85      0.70      0.77       228\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1000\n",
      "   macro avg       0.77      0.77      0.77      1000\n",
      "weighted avg       0.79      0.78      0.78      1000\n",
      " samples avg       0.78      0.78      0.78      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7228\n",
      "softmax loss:  3.62084087252217e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7799\n",
      "softmax loss:  1.917585498575037e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8019\n",
      "softmax loss:  1.001024824261873e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8182\n",
      "softmax loss:  5.357968292509009e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8138\n",
      "softmax loss:  6.170754618629236e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8138\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1996\n",
      "           1       0.88      0.94      0.90      1853\n",
      "           2       0.68      0.80      0.74      1651\n",
      "           3       0.58      0.64      0.61      1809\n",
      "           4       0.94      0.73      0.82      2691\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     10000\n",
      "   macro avg       0.81      0.82      0.81     10000\n",
      "weighted avg       0.83      0.81      0.82     10000\n",
      " samples avg       0.81      0.81      0.81     10000\n",
      "\n",
      "accuracy on test data:  0.778\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       230\n",
      "           1       0.82      0.89      0.85       182\n",
      "           2       0.58      0.72      0.65       160\n",
      "           3       0.60      0.56      0.58       201\n",
      "           4       0.86      0.70      0.77       227\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1000\n",
      "   macro avg       0.77      0.77      0.77      1000\n",
      "weighted avg       0.79      0.78      0.78      1000\n",
      " samples avg       0.78      0.78      0.78      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128, 64]\n",
      "Convergence criteria satisfied!\n",
      "epoch 25\n",
      "accuracy on train data:  0.2091\n",
      "softmax loss:  0.0002359463199165115\n",
      "\n",
      "\n",
      "accuracy on train data:  0.2091\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "   micro avg       0.21      0.21      0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      " samples avg       0.21      0.21      0.21     10000\n",
      "\n",
      "accuracy on test data:  0.187\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "   micro avg       0.19      0.19      0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      " samples avg       0.19      0.19      0.19      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512,256], [512,256,128], [512,256,128,64]]\n",
    "network_depth = [1,2,3,4]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    NN = NeuralNetwork(n = 1024, n_hidden_nodes = hidden_layer , r = 5, M = 32)\n",
    "    NN.train(X_train, y_train_onehot, epoch_mode= False, activation=\"sigmoid\", alpha = 0.01, stopping_threshold = 1.0e-06, printafter=50)\n",
    "    y_pred_train, _ = NN.predict(X_train)\n",
    "    y_pred_test, _ = NN.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAFNCAYAAACjRAOYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABEmklEQVR4nO3de5yN5frH8c9lnCLJqRPC7iyHkaEDRaKckkOiVEjpHNXWaVfbrq1dqTZt/SqSSg5FO7GRQkSqjdIuSqWIUs4iOV+/P+5lWsYMg1meNTPf9+u1XrPW89zPs661Wo3v3M+97tvcHRERERE59ApEXYCIiIhIfqUgJiIiIhIRBTERERGRiCiIiYiIiEREQUxEREQkIgpiIiIiIhFREBMRzOxoM3vfzDaY2ZNR1yP7z8x6m9mrB3hsFzObmdM1xc7d0MyWJeLcInmBgphILmBm08xsrZkVSdBTdAdWAUe4+51mVs3MJpnZKjPTZIMHKJEBJ1mZmZvZiVHXIZJbKIiJJDkzqwycCzjQKkFPUwlY4H/M8LwNeB3olqDn228W6HdWJsysYNQ1iMiB0S81keR3NfAR8BLQGcDMipjZOjOrtquRmZUzs9/N7KjY47vMbLmZ/WRm12bVU2Fmu857l5ltNLPG7r7Q3QcD8/enUDO728x+jF3iXGhmF8S2p5jZfWa2KLZvrplVjO07x8xmm9n62M9z4s43zcz6mNkHwCbgT2Z2qpm9a2ZrYs9xWRa1dDCzORm23W5mY2P3m5vZglg9P5rZn7M4Txczm2lmT8R6Jb83s2Zx+0ua2eDYe/2jmf099npPA54Dzo69r+vMrErsZ4HYsYPMbEXcuYaaWc/Y/ePMbGzsdX5rZtfFtettZqPN7FUz+xXokqHmQmY2wszeMLPCmbymMrFz/2pm/wVOyLA/y/fYzF4ys+di+zeY2XQzqxTb936s2Wex19wh7rg7zWxF7H3qmtl7LZIvubtuuumWxDfgW+AmoDahp+ro2PYXgT5x7W4G3o7dbwr8DJwOFANeJfSonZjFc7wE/D2T7SeGXxPZqvMUYClwXOxxZeCE2P1ewOexNgbUBMoApYG1wFVAQeDy2OMyseOmAT/EXkdBoGTsObrGHtciXFKtmkk9xYANwElx22YDHWP3lwPnxu6XAs7I4nV1ib3v1wEpwI3AT4DF9r8JPA8UB44C/gtcH3fszAzn+wGoHbu/EPgOOC1uX63Y/feB/wOKAqnASqBRbF/vWE2tCX9QHxbb9mrs/vjYf9OULF7TSEKPZ3GgGvDjrjpj27J8j2Pn3QCcBxQB+se/RjJ8zoCGwHbgIaAQ0JwQqktF/f+Wbrolw009YiJJzMzqEy4bvu7uc4FFwBWx3cOBjnHNr4htA7gMGOLu8919E+Ef6UTbQfiHuaqZFXL3xe6+KLbvWuB+Dz1t7u6fuftqoAXwjbsPdfft7j4C+Aq4OO68L8Vex3ZCwFzs7kNi7T8F3gDaZywm9rrfIoQ7zOwk4FRgbKzJtlitR7j7Wnf/ZC+vbYm7D3L3HcDLwLHA0WZ2NCFY9HT339x9BfBPdv/vktF0oIGZHRN7PDr2uApwBKE3qSJQD7jb3Te7+zzgBULv6C4fuvsYd9/p7r/Hth0BvE34nHSN1bsbM0sB2gEPxmr+IvaadmnJvt/j8e7+vrtvAf5C6PWruJfXvA14yN23ufsEYCMhlIvkewpiIsmtM/COu6+KPR4e2wbwHlDMzM60MI4sldA7A3AcoVdjl/j7CeHu3wI9CaFvhZmNNLPjYrsrEsJBRscBSzJsWwKUj3scX3sl4MzY5b11ZrYO6AQcQ+aGEwtihKA6JhbQIISR5sCS2OW1s/fy8n7edSfu+MNj9RQClsfV8zyhZywr0wm9ROcRer2mAQ1itxnuvpPwvqxx9w1xx+3tfdnlLKAG8Ki7Z/Uli3KEnq744+P/G2TnPU4/1t03AmtiNWdldSxI77KJ8P6J5Hsa4CmSpMzsMELPVoqZ7QoCRYAjzaymu39mZq8TgsYvwH/i/uFeDlSIO93eeityjLsPB4ab2RGEQPIY4bLjUsI4pC8yHPIT4R/+eMcTenXSTxt3fykw3d2bZLOkd4FyZpZKeJ9uj6t1NnCJmRUCbiFcqtvf92kpsAUomyFoZFb7LtOBvsCy2P2ZhLFkm2OPIbwvpc2sRNx/0+MJlxD3du53gP8BU8ysobv/kkmblYRLhRUJvY+7zh3/mvb1Hqe/T2Z2OOES8097aS8iWVCPmEjyak243FeV0NuVCpwGzOCPS1TDgQ6EHovhcce+DnQ1s9PMrBjwwP48sQVFgcKxx0VtH1NnmNkpZtYo1m4z8DuwM7b7BeBhMzspdu4aZlYGmACcbGZXmFnB2ODuqsB/snia/8TaXxUbkF7IzOrEBsbvwd23AaMIwac0IZhhZoXNrJOZlYy1+TWu1mxz9+WE8POkmR1hZgXM7AQzaxBr8gtQIX7AvLt/E3tvriQEnl9j7doRC2LuvhSYBfwj9t7XIHyDdZ/zhLn744TPwhQzK5vJ/h3Av4HeZlbMzKryRy8rZO89bm5m9WOv62Hgo1jNu17zn/ZVp4gECmIiyaszYZzXD+7+864bMADoZGYF3f1j4DfCZaGJuw5094nA04TLl98SvnUJofcmOyoRwsKub03+ThhYvjdFgEcJA7t/Jlyeuze27ylCOHyHEHoGA4fFxom1BO4EVgN3AS3jLsXuJtY7dCFhDNZPsed5LPbcWRkONAZGZei1ugpYHPvW4Q2EMHsgriYE1gWELxqMJowhA5hKeA9/NrP41zSdcLluadxjA+LHqV1O+MLDT4RLzn9198nZKcjdHwbGAJPNrHQmTW4hXBr8mTD4fkjcsdl5j4cDfyVckqxNCJW79AZejl3WzPQbrSLyh13f+hGRPCzWm/EFUCSLS2gi2WJhupNl7n5/1LWI5AXqERPJo8ysjYX5xkoRejTGKYSJiCQXBTGRvOt6YAXh24o7CPNfHRQzOz42UWdmt+P3fQYREYmnS5MiIiIiEVGPmIiIiEhEFMREREREIpIrJ3QtW7asV65cOeoyRERERPZp7ty5q9y9XGb7cmUQq1y5MnPmzIm6DBEREZF9MrOMS7ml06VJERERkYgoiImIiIhEREFMREREJCK5coyYiIjIgdq2bRvLli1j8+bNUZcieUzRokWpUKEChQoVyvYxCmIiIpKvLFu2jBIlSlC5cmXMLOpyJI9wd1avXs2yZcuoUqVKto/TpUkREclXNm/eTJkyZRTCJEeZGWXKlNnvnlYFMRERyXcUwiQRDuRzpSAmIiIiEhEFMRERkQiMGTMGM+Orr77KkfNt2bKFxo0bk5qaymuvvcaAAQM48cQTMTNWrVqVI8+R08aMGcOCBQv2+7ixY8fy6KOPHvDz9u7dmyeeeGKvbQ60tv2lICYiIrIXw4ZB5cpQoED4OWxYzpx3xIgR1K9fnxEjRuTI+T799FMA5s2bR4cOHahXrx6TJ0+mUqVKOXL+/bF9+/Zstdtb2NnbOVq1asU999xzQLVll4KYiIhIxIYNg+7dYckScA8/u3c/+DC2ceNGZs6cyeDBgxk5ciQAb7/9Nu3bt09vM23aNFq2bAnA4MGDOfnkk6lbty7XXXcdt9xyy27nW7FiBVdeeSWzZ88mNTWVRYsWUatWLbK7LvP06dNJTU0lNTWVWrVqsWHDBgAee+wxqlevTs2aNdODz7x58zjrrLOoUaMGbdq0Ye3atQA0bNiQnj17kpaWRv/+/Zk7dy4NGjSgdu3aXHTRRSxfvny355w1axZjx46lV69e6TVnPMe4ceM488wzqVWrFo0bN+aXX34B4KWXXkp/D7p06cJtt93GOeecw5/+9CdGjx6d6Wvs06cPJ598MvXr12fhwoXp2wcNGkSdOnWoWbMm7dq1Y9OmTZnWllm7HOHuue5Wu3ZtFxERORALFixIv9+jh3uDBlnfihRxDxFs91uRIlkf06PHvmt49dVX/ZprrnF397PPPtvnzJnj27Zt84oVK/rGjRvd3f2GG27woUOH+o8//uiVKlXy1atX+9atW71+/fp+880373HO9957z1u0aLHH9kqVKvnKlSv3Wk/Lli195syZ7u6+YcMG37Ztm0+YMMHPPvts/+2339zdffXq1e7uXr16dZ82bZq7uz/wwAPeI/aCGzRo4DfeeKO7u2/dutXPPvtsX7Fihbu7jxw50rt27brH83bu3NlHjRqV/jj+HO7ua9as8Z07d7q7+6BBg/yOO+5wd/chQ4akvwedO3f2Sy+91Hfs2OHz58/3E044YY/nmTNnjlerVs1/++03X79+vZ9wwgnet29fd3dftWpVeru//OUv/vTTT2daW1btMor/fO0CzPEsMo3mERMREcnCli37tz27RowYQY8ePQDo2LEjI0aMoHbt2jRt2pRx48Zx6aWXMn78eB5//HGmTJlCgwYNKF26NADt27fn66+/PrgCMqhXrx533HEHnTp1om3btlSoUIHJkyfTtWtXihUrBkDp0qVZv34969ato0GDBgB07tx5t168Dh06ALBw4UK++OILmjRpAsCOHTs49thjs1XLrnNAmPOtQ4cOLF++nK1bt2Y5P1fr1q0pUKAAVatWTe81izdjxgzatGmT/lpatWqVvu+LL77g/vvvZ926dWzcuJGLLroo0+fIbrv9pSAmIiL5Vr9+e99fuXK4HJlRpUowbdqBPeeaNWuYOnUqn3/+OWbGjh07MDP69u1Lx44dGTBgAKVLlyYtLY0SJUoc2JPsp3vuuYcWLVowYcIE6tWrx6RJkw7oPMWLFwfC1bbTTz+dDz/88IDPAXDrrbdyxx130KpVK6ZNm0bv3r0zPaZIkSLp90MHVPZ16dKFMWPGULNmTV566SWmZfEfNrvt9pfGiImIiGShTx+IdaKkK1YsbD9Qo0eP5qqrrmLJkiUsXryYpUuXUqVKFWbMmEGDBg345JNPGDRoEB07dgSgTp06TJ8+nbVr17J9+3beeOONg3hFmVu0aBHVq1fn7rvvpk6dOnz11Vc0adKEIUOGpI+FWrNmDSVLlqRUqVLMmDEDgKFDh6b3jsU75ZRTWLlyZXoQ27ZtG/Pnz9+jXYkSJdLHo2Vm/fr1lC9fHoCXX375gF/feeedx5gxY/j999/ZsGED48aNS9+3YcMGjj32WLZt28awuMF/GWvLqt3BUhATERHJQqdOMHBg6AEzCz8HDgzbD9SIESNo06bNbtvatWvHiBEjSElJoWXLlkycODF9oH758uW57777qFu3LvXq1aNy5cqULFlyn8/z9NNPU6FCBZYtW0aNGjW49tprs2zbr18/qlWrRo0aNShUqBDNmjWjadOmtGrVirS0NFJTU9One3j55Zfp1asXNWrUYN68eTz44IN7nK9w4cKMHj2au+++m5o1a5KamsqsWbP2aNexY0f69u1LrVq1WLRo0R77e/fuTfv27alduzZly5bd52vOyhlnnEGHDh2oWbMmzZo1o06dOun7Hn74Yc4880zq1avHqaeemmVtWbU7WLa/XXjJIC0tzefMmRN1GSIikgt9+eWXnHbaaVGXsV82btzI4Ycfzvbt22nTpg3XXHPNHmFOkkNmny8zm+vuaZm1V4+YiIhIkuvduzepqalUq1aNKlWq0Lp166hLkhyiwfoiIiJJbl+zwGfXkCFD6N+//27b6tWrxzPPPJMj55f9l9AgZmYvAi2BFe5eLZP9BvQHmgObgC7u/kkiaxIREcmvunbtSteuXaMuQ+Ik+tLkS0DTvexvBpwUu3UHnk1wPSIiIiJJI6FBzN3fB9bspcklwCuxiWc/Ao40s+zN+CYiIiKSy0U9WL88sDTu8bLYNhEREZE8L+oglm1m1t3M5pjZnJUrV0ZdjoiIiMhBizqI/QhUjHtcIbZtD+4+0N3T3D2tXLlyh6Q4ERERhg0Lax0VKBB+5tCs6mPGjMHM+Oqrr3LkfFu2bKFx48akpqby2muvMWDAAE488UTMjFWrVuXIc+S0MWPGsGDBggM6dt68eUyYMCFbbRs2bMi+5h/t169f+ioCh1LUQWwscLUFZwHr3X15xDWJiIgEw4ZB9+5hwUn38LN79xwJYyNGjKB+/fqMGDEiBwqFTz/9FAgBpUOHDtSrV4/JkydTqVKlHDn//ti+fXu22h2qIJYdeTKImdkI4EPgFDNbZmbdzOwGM7sh1mQC8B3wLTAIuCmR9YiIiOymZ09o2DDrW7dukPEf502bwvasjunZc59Pu3HjRmbOnMngwYMZOXIkAG+//Tbt27dPbzNt2rT0ZY4GDx7MySefTN26dbnuuuu45ZZbdjvfihUruPLKK5k9ezapqaksWrSIWrVqUbly5Wy9DdOnTyc1NZXU1FRq1aqVvsbiY489RvXq1alZsyb33HMPEALQWWedRY0aNWjTpg1r164FQq9Tz549SUtLo3///sydO5cGDRpQu3ZtLrroIpYv372fZdasWYwdO5ZevXql17xo0SKaNm1K7dq1Offcc9N7C0eNGkW1atWoWbMm5513Hlu3buXBBx/ktddeS+8BjPf777/TsWNHTjvtNNq0acPvv/+evu/GG28kLS2N008/nb/+9a9AWA7qp59+4vzzz+f888/Psl1CuHuuu9WuXdtFREQOxIIFC/540KOHe4MGWd9CP1jmt6yO6dFjnzW8+uqrfs0117i7+9lnn+1z5szxbdu2ecWKFX3jxo3u7n7DDTf40KFD/ccff/RKlSr56tWrfevWrV6/fn2/+eab9zjne++95y1atNhje6VKlXzlypV7radly5Y+c+ZMd3ffsGGDb9u2zSdMmOBnn322//bbb+7uvnr1and3r169uk+bNs3d3R944AHvEXu9DRo08BtvvNHd3bdu3epnn322r1ixwt3dR44c6V27dt3jeTt37uyjRo1Kf9yoUSP/+uuv3d39o48+8vPPP9/d3atVq+bLli1zd/e1a9e6u/uQIUMyfR/c3Z988sn05/vss888JSXFZ8+evdvr2L59uzdo0MA/++yzTN+nrNrty26frxhgjmeRaTSzvoiI5F/9+u19f+XK4XJkRpUqwbRpB/y0I0aMoEePHkBYXHrEiBHUrl2bpk2bMm7cOC699FLGjx/P448/zpQpU2jQoAGlS5cGoH379nz99dcH/NyZqVevHnfccQedOnWibdu2VKhQgcmTJ9O1a1eKFSsGQOnSpVm/fj3r1q2jQYMGAHTu3Hm3XrwOHToAsHDhQr744guaNGkCwI4dOzj22L3PTrVx40ZmzZq12/m2bNmSXl+XLl247LLLaNu27T5fz/vvv89tt90GQI0aNahRo0b6vtdff52BAweyfft2li9fzoIFC3bbv7/tDpaCmIiISFb69AljwuIvTxYrFrYfoDVr1jB16lQ+//xzzIwdO3ZgZvTt25eOHTsyYMAASpcuTVpaGiVKlMiBF7Fv99xzDy1atGDChAnUq1ePSZMmHdB5ihcvDoSrbaeffjoffvhhto/duXMnRx55JPPmzdtj33PPPcfHH3/M+PHjqV27NnPnzj2g+r7//nueeOIJZs+eTalSpejSpQubN28+4HY5IerB+iIiIsmrUycYODD0gJmFnwMHhu0HaPTo0Vx11VUsWbKExYsXs3TpUqpUqcKMGTNo0KABn3zyCYMGDaJjx44A1KlTh+nTp7N27Vq2b9/OG2+8kVOvLt2iRYuoXr06d999N3Xq1OGrr76iSZMmDBkyJH0A+5o1ayhZsiSlSpVixowZAAwdOjS9dyzeKaecwsqVK9OD2LZt25g/f/4e7UqUKJE+Hu2II46gSpUqjBo1Cghh7rPPPkuv78wzz+Shhx6iXLlyLF26dLdjMzrvvPMYPnw4AF988QX/+9//APj1118pXrw4JUuW5JdffmHixImZ1rK3djlNQUxERGRvOnWCxYth587w8yBCGITLkm3atNltW7t27RgxYgQpKSm0bNmSiRMnpg/UL1++PPfddx9169alXr16VK5cmZIlS+7zeZ5++mkqVKjAsmXLqFGjBtdee22Wbfv160e1atWoUaMGhQoVolmzZjRt2pRWrVqRlpZGampq+sLjL7/8Mr169aJGjRrMmzePBx98cI/zFS5cmNGjR3P33XdTs2ZNUlNTmTVr1h7tOnbsSN++falVqxaLFi1i2LBhDB48mJo1a3L66afz1ltvAdCrVy+qV69OtWrVOOecc6hZsybnn38+CxYsyHSw/o033sjGjRs57bTTePDBB6lduzYANWvWpFatWpx66qlcccUV1KtXL/2Y7t2707RpU84///y9tstpFsaQ5S5paWm+r/lAREREMvPll19y2mmnRV3Gftm4cSOHH34427dvp02bNlxzzTV7hDlJDpl9vsxsrrunZdZePWIiIiJJrnfv3qSmplKtWjWqVKlC69atoy5JcogG64uIiCS5XZcFD9aQIUPo37//btvq1avHM888kyPnl/2nICYiIpJPdO3ala5du0ZdhsTRpUkREcl3cuP4aEl+B/K5UhATEZF8pWjRoqxevVphTHKUu7N69WqKFi26X8fp0qSIiOQru6Z0WLlyZdSlSB5TtGhRKlSosF/HKIiJiEi+UqhQIapUqRJ1GSKALk2KiIiIREZBTERERCQiCmIiIiIiEVEQExEREYmIgpiIiIhIRBTERERERCKiICYiIiISEQUxERERkYgoiImIiIhEREFMREREJCIKYiIiIiIRURATERERiYiCmIiIiEhEFMREREREIqIgJiIiIhIRBTERERGRiCiIiYiIiEREQUxEREQkIgpiIiIiIhFREBMRERGJiIKYiIiISEQUxDIYNgwqV4YCBcLPYcOirkhERETyqoJRF5BMhg2D7t1h06bweMmS8BigU6fo6hIREZG8SUEszl/+8kcI22XTJrjhBvjwQyhUCAoXzvzn3vbtb5tChUKPnIiIiORtCmJxfvgh8+0bN8KIEbBtW7ht3Qo7dya2lpSUgwt0iQiICpEiIiI5S0EszvHHh8uRGVWqBIsX775tx47dg1n8z8y2ZfUzp9ps3Jj98yR7iIwqaOaWEDlsWOi9/eGH8Jnt00eXzkVEcisFsTh9+uw+RgygWLGwPaOUlHArWvTQ1ZdTDjZE5lSb/BoiD6bNhAkhhP3+e6h3yRK47rrw3lx++R+fS7NwE4mK/mAQyR5z96hr2G9paWk+Z86chJxbvzySW7KEyANpk+gQmVGBAuGWknLgPw/m2ENxzmQ9V1bnzC/hOOMXnyD8UTtwoH6fSv5kZnPdPS3TfYkOYmbWFOgPpAAvuPujGfYfD7wMHBlrc4+7T9jbORMZxEQSJbMQub+Xsa+4IuvzP/JIeI6dO3PmZ06e62DOmZeYJXdQzKlz9e4Na9fu+fozG+Yhkh9EFsTMLAX4GmgCLANmA5e7+4K4NgOBT939WTOrCkxw98p7O6+CmORXlStnfxxjXuGe3EExP57zQP/ZMDv0PcMiyWBvQSzRY8TqAt+6+3exQkYClwAL4to4cETsfkngpwTXJJJr7c84xrzCDApqNGtScQ+BKqugVrMmLFu253HHH3/oaxVJdon+jlh5YGnc42WxbfF6A1ea2TJgAnBrgmsSybU6dQrjbCpVCgGlUiWNu5FDzyxcgixUKHxhqVgxKFECSpaE0qXh0UfDtngFC+btPxhEDlQyfFn/cuAld68ANAeGmtkedZlZdzObY2ZzVq5ceciLFEkWnTqFy5A7d4afCmGSbDL+wXD44bB9O/yk6x0ie0h0EPsRqBj3uEJsW7xuwOsA7v4hUBQom/FE7j7Q3dPcPa1cuXIJKldERHJC/B8M69bBZZfBXXfBiy9GXZlIckl0EJsNnGRmVcysMNARGJuhzQ/ABQBmdhohiKnLS0Qkj0hJgaFD4cILw7x3Y8ZEXZFI8khoEHP37cAtwCTgS+B1d59vZg+ZWatYszuB68zsM2AE0MWjnNxs2LDw1bQCBcLPYcMiK0VEJK8oXBjeeAPq1IGOHeG996KuSCQ5aELXeJqFUEQkoVavhvPOg6VLQxirXTvqikQSL9IJXRMhYUEsq0maUlKgQoU/Zi2MvxUsmPn2fd0O5LhD+VwHclx+mjo8Slr+QXKDvXxOf/wR6tWD336DmTPhlFMirlUkwaKcRyx3+eGHzLfv2AENGvwx62H8bfv2Pbdt2wabN2fefm/HZXXLTTMgHkjgyw3B9FCH4JSUzENtxl7bJUvCY1AYk+Sxj89p+fLwzjtQv34YN/bBB+FvXZH8SD1i8ZJ12vJdsyfub4A7kNB3sMcdyuc60ONyi/i1Y3bdNm7MPJinpMAxx2T/3Afac6njkuc5k/m4774L/29mlOF36SefQMOGIYTNmAFlyhxYaSLJTj1i2ZWs05bvmj0xJSWMeJUDFx9qkzksZnVM//6Zv64dO6Bp0+y/Bwf63um46I+L4jn397ivv858e4arDmecAWPHho9u8+YwZUqYc0wkP1EQi7fr0o7G3+Rd8aE2NxozJute2xdeOOTliGTqww8z/5xmssZRw4bw2mvQtm24jRsHRYokvkSRZJEMM+snF01bLsmsT589145Jhl5bkXiZfU4Bbrwx0+aXXAKDB8O778JVV+WuEQQiB0tBTCQ30WKTkhtk/JwedxwceSQ89RR89VWmh3TpAk8+CaNGwc03H9yVW5HcRIP1RUQk8b76Knz7vGBBeP99OOGETJvddx/84x/hpzp6Ja/Y22B99YiJiEjinXpqGI2/ZQs0apT5GDJC+LruOnjkkdCBJpLXKYiJiMihUa1aGAj2668hjC1btkcTM3j2WWjXDu68E15+OYI6RQ4hBTERETl0atWCSZNg5Uq44AL4+ec9mqSkhDlhL7gAunULU1yI5FUKYiIicmjVrQsTJ4a1ji64IISyDIoUgTffDHONXXYZTJ8eQZ0ih4CCmIiIHHr16oVJw777Dpo0gTVr9mhSogRMmABVqkCrVvDppxHUKZJgCmIiIhKN88+Ht96CL7+Eiy6C9ev3aFK2bFiXsmTJMAP/N99EUKdIAimIiYhIdC68EN54A+bNg2bNYMOGPZpUrBjC2M6dofPsxx8PfZkiiaIgJiIi0WrZMqxz9N//hvvx6/3GnHpqGFa2enXoPMvkSqZIrqQgJiIi0WvbFl59FWbODGsebd68R5O0tHAl85tvoEUL+O23COoUyWEKYiIikhw6doQXX4TJk8NEYlu37tGkUSMYOTJ0nmXRRCRXURATEZHk0bkzPP98+Lpkhw6wbdseTdq0gUGDwnRkV1+tRcIldysYdQEiIiK76d49LIV0221w5ZVhdteCu/9zdc01YbzYXXdBmTIwYECYlV8kt1EQExGR5HPrrSGM9eoVZnd96SUosPtFnF69YNUqePzxEMYeeiiaUkUOhoKYiIgkpz//OQzaf+CBEMaef36PMPboo6Fn7OGHQxjr0SOiWkUOkIKYiIgkr/vvDz1jf/97CGP/+tdu1yDN4LnnwnQWPXuGMHblldGVK7K/FMRERCS5PfRQ6Bl74okQxp54YrcwVrAgDB8OzZtDly5QqlSY3kIkN1AQExGR5GYWBoJt3gxPPQVFi4YesrgwVrQojBkTpre49NIwE/+550ZXskh2afoKERFJfmbQvz9cdx088kgIYhkccUSYff/44+Hii+GzzyKoU2Q/KYiJiEjuUKBAGBDWuTM8+GDoJcugXDl4910oUSIshbRoUQR1iuyHbAUxMzvazAab2cTY46pm1i2xpYmIiGRQoAAMHhxm4b/77tBLlsHxx4dLk9u3h0XCly+PoE6RbMpuj9hLwCTguNjjr4GeCahHRERk71JS4JVXwhT7PXuGXrIMTjstTM6/YkXoGVu79tCXKZId2Q1iZd39dWAngLtvB7SohIiIRKNQobDoZIsWcOONMGTIHk3q1g0D+BcuhJYtYdOmQ1+myL5kN4j9ZmZlAAcws7OA9QmrSkREZF8KF4bRo8P1x27dwhwWGTRuHDZ/9FH4NmUmS1eKRCq7QewOYCxwgpl9ALwC3JqwqkRERLJj17wVDRqEFcBHj96jSbt24erlxIlhnrGdOw95lSJZ2uc8YmaWAjSI3U4BDFjo7vq7QkREolesGIwbFwaDXX55mPT14ot3a3LddWEppHvvhdKl4emntUi4JId99oi5+w7gcnff7u7z3f0LhTAREUkqhx8eRufXqhWuQU6atEeTu++GO++EAQO0QLgkj+xemvzAzAaY2blmdsauW0IrExER2R8lS4YAVrUqtG4NU6futtsM+vYNlyd79w6BTCRq2V3iKDX2M/5vCAca5Wg1IiIiB6NUqTCja8OG4fLkpElQv376bjMYNCgsEn7rreEy5RVXRFeuSLaCmLufn+hCREREckTZsjBlShjA37x5CGZnnpm+u2DBMPNFs2Zhkv5SpcJ9kShkd2b9kmb2lJnNid2eNLOSiS5ORETkgBx9dAhjRx0VBvF/8sluuw87DN56C6pXD9+qnDUrojol38vuGLEXgQ3AZbHbr8Ces+eJiIgki/LlwzixI48Mc419/vluu0uWhLffhgoVwrywGXaLHBLZDWInuPtf3f272O1vwJ8SWZiIiMhBO/740DN22GFwwQXw5Ze77T7qqLAuZbFioePsu+8iqlPyrewGsd/NLH20o5nVA35PTEkiIiI56IQTQhgrUCCEsW++2W135cohjG3eDBdeCD//HE2Zkj9lN4jdCDxjZovNbDEwALghOweaWVMzW2hm35rZPVm0uczMFpjZfDPbc40KERGRg3HKKSGMbdsGjRrB99/vtvv008M0ZD//DE2bwrp10ZQp+U+2gpi7z3P3mkANoIa713L3z/Z1XGxW/meAZkBV4HIzq5qhzUnAvUA9dz8d6Ll/L0FERCQbTj8dJk+G334LPWNLl+62+6yz4N//hgULwswXWiRcDoXsfmvyETM70t1/dfdfzayUmf09G4fWBb6NjSvbCowELsnQ5jrgGXdfC+DuK/bnBYiIiGRbzZphbrHVq0MYW758t90XXgivvgoffAAdOmiRcEm87F6abObu63Y9iIWm5tk4rjwQ/yfHsti2eCcDJ5vZB2b2kZk1zWZNIiIi+69OnbAC+E8/hTC2Yve//y+7DP7v/+A//4FrrtEi4ZJY2Q1iKWZWZNcDMzsMKLKX9vujIHAS0BC4HBhkZkdmbGRm3XfNY7Zy5cocemoREcmXzjkHxo+HxYuhcePQQxbnhhvg738PvWN33AHu0ZQpeV92g9gwYIqZdTOzbsC7wMvZOO5HoGLc4wqxbfGWAWPdfZu7fw98TQhmu3H3ge6e5u5p5cqVy2bZIiIiWWjQAMaOha+/DtckM4zQv+8+6NkT+veHPn0iqVDygewO1n8M+DtwWuz2sLs/no1DZwMnmVkVMysMdATGZmgzhtAbhpmVJVyq1EwuIiKSeI0bhxH6n38evi65YUP6LjN48km46ip44AF49tkI65Q8K7uD9YsD77j7n4FBQBEzK7Sv49x9O3ALMAn4Enjd3eeb2UNm1irWbBKw2swWAO8Bvdx9deZnFBERyWHNm8Prr8OcOWGK/d9+S99VoAAMHgwtW8LNN8Nrr0VYp+RJ5tm48G1mc4FzgVLATGAOsNXdOyW2vMylpaX5nDlzonhqERHJq157Da64Aho2DCP1Dzssfdfvv4eZ9z/6CMaNC/dFssvM5rp7Wmb7sjtGzNx9E9AWeNbd2wOn51SBIiIikevQAV56Cd57D9q2hS1b0ncddlgYTla1atj10UfRlSl5S7aDmJmdDXQCxse2pSSmJBERkYhcdRUMHBhWA7/sst0mEjvyyLD52GPD1cz586MrU/KO7AaxHoTZ79+MjfH6E2E8l4iISN5y7bUwYEDoAuvUCbZvT991zDHw7rtQtGj4ouXixdGVKXlDdr81+b67t4p9e5LYTPm37dpvZv9KVIEiIiKH3M03h69MjhoFXbrAjh3pu6pUCZPzb9oETZrAL79EV6bkftntEduXejl0HhERkeRwxx3wyCMwbBh0777bFPvVq4dFwn/6CZo1g/XrI6xTcrWcCmIiIiJ5z733woMPwosvwi237DbF/tlnwxtvhCnIWrUK36wU2V8KYiIiInvTuzfcdVeY0TXDekdNm8Irr8CMGdCx427DyUSypWAOncdy6DwiIiLJxQwefTRMZ9GvHxQpAv/4R9gOXH45rF0bhpVde23oPCugbg7JppwKYv1z6DwiIiLJxwz++c8Qxh57LEws9te/pu++6SZYtSpsKlMGnngiPaeJ7NUBBzEzG+ju3QHc/aUcq0hERCQZmcEzz4Qw1rt36Bm755703Q88EMLYU09B2bJheJnIvuw1iJlZ6ax2Ac1zvhwREZEkVqAADBoUwti994YwdvvtQMhp/frB6tVw332hZ6x792jLleS3rx6xlcASdh8D5rHHRyWqKBERkaSVkgIvvxzC2B13hDB2001AyGkvvRTGjN1wA5QuDZdeGm25ktz2FcS+Ay5w9x8y7jCzpYkpSUREJMkVLAjDh4eUdfPNIYx16wZAoUIwenSYef+KK6BkyTDxq0hm9vW9jn5AqSz2PZ6zpYiIiOQihQuHmfcvugiuuw5efTV9V7FiMG4cnHoqtGkD//1vhHVKUttXEFvh7p+ZWZWMO9xdyxqJiEj+VqQIvPkmNGwInTuHYBZTqlRYCumoo8Ls+19+GV2Zkrz2FcR2fefjjUQXIiIikisddljo/jrnnHAt8q230ncde2xYJLxw4XB5csmSCOuUpLSvILbazN4BqpjZ2Iy3Q1GgiIhI0iteHMaPh9q1oX37sBBlzAknhJ6xjRvDuLGVKyOsU5LOvgbrtwDOAIYCTya+HBERkVzqiCPg7bfhggugbdsQzC64AIAaNeA//wlBrFkzmDo1NBfZa4+Yu29194+Ac9x9esbbIapRREQkdzjySHjnHTj5ZLj4Ynj//fRd9euHb1N+9hm0bg2bN0dWpSSRbK2G5e7qSBUREcmOMmVg8mSoVAlatIAPP0zf1bx5mGfsvffCGpVaJFy0LKmIiEhOO+oomDIFjjkGmjaFOXPSd3XqBP37w5gxcP314B5dmRI9BTEREZFEOO64MBisdOkwOOyzz9J33XYbPPggvPjibstVSj6UrUW/zezpTDavB+a4+1uZ7BMREZGKFUMYO+88aNwYpk+HqlWBsG74qlXw+OPhauZdd0VbqkQjuz1iRYFU4JvYrQZQAehmZv0SUpmIiEheUKVKCGMFC4ZvUX79NRAWCf/Xv6BjR7j7bnjhhYjrlEhkq0eMELzqufsOADN7FpgB1Ac+T1BtIiIiecNJJ4UxYw0bQqNG4duUf/oTBQqE9cPXrg3jxUqXDjNfSP6R3R6xUsDhcY+LA6VjwWxLjlclIiKS11StGr5N+fvvIYz98AMQZt1/4w2oWzd8k3Lq1IjrlEMqu0HscWCemQ0xs5eAT4G+ZlYcmJyo4kRERPKUGjXCPGPr1oUw9tNPwB8T8590ElxyyW5fspQ8zjyb35s1s2OBurGHs939p4RVtQ9paWk+R59SERHJrT76KCw+WaECTJsGRx8NhFxWr15YDmnGDDj11GjLlJxhZnPdPS2zfdnqETOzcUBDYLK7vxVlCBMREcn1zjorrEf5ww/h25SrVgFhxot334UCBcKMF0uXRlynJFx2L00+AZwLLDCz0WZ2qZkVTWBdIiIiedu558K4cfDttyF1rV0LwIknhkXC168Pm2MZTfKo7C5xNN3dbwL+BDwPXAasSGRhIiIieV6jRvDmmzB/fpiB/9dfAUhNDRlt8eKwLNKGDZFWKQmU7Zn1zewwoB1wA1AHeDlRRYmIiOQbTZvC66/DJ5+E1LVxIxDmgN21uU0b2KI5CvKk7I4Rex34EmgEDABOcPdbE1mYiIhIvnHJJTB8eFgg/OKLYdMmINx98cUwBVmnTrBjR8R1So7Lbo/YYEL4usHd3wPOMbNnEliXiIhI/tK+PbzySlgGqU0b2LwZgKuvhn/+M8w1duONWiQ8r8nWzPruPsnMapnZ5YTxYd8D/05oZSIiIvlNp07hGmS3biGYvfEGFC5Mz55h0H6fPlC2LDzySNSFSk7ZaxAzs5OBy2O3VcBrhLnHzj8EtYmIiOQ/11wTwthNN4Wp9l97DQoW5OGHQxj7xz/CIuF33hl1oZIT9tUj9hVhTcmW7v4tgJndnvCqRERE8rMbbwxh7Pbbw7XJoUOxlBSeeQbWrIE//zmsS9m1a9SFysHaVxBrC3QE3jOzt4GRgCW8KhERkfyuZ88Qxu65B4oUgcGDSUkpwNChYYWka6+FUqWgdeuI65SDstfB+u4+xt07AqcC7wE9gaPM7Fkzu/AQ1CciIpJ/3X039O4NL70ULlW6U6QI/PvfkJYGHTuGFZIk98ruhK6/uftwd78YqEBY9PvuhFYmIiIi8OCDcO+98PzzoZfMncMPDysknXACtGoV5hqT3CnbE7ru4u5r3X2gu1+QnfZm1tTMFprZt2Z2z17atTMzN7NMF8UUERHJl8zC1yVvvx2efjr0krlTpgy8804YK9a0KXz9ddSFyoHY7yC2P8wsBXgGaAZUBS43s6qZtCsB9AA+TmQ9IiIiuZIZPPlkuDzZty/89a8AlC8fwhhAkyawbFmENcoBSWgQA+oC37r7d+6+lTDY/5JM2j0MPAZsTnA9IiIiuZMZ/OtfYY6xhx8OvWTAySfD22+HNcMvughWr464TtkviQ5i5YGlcY+XxbalM7MzgIruPj7BtYiIiORuBQqEsWJXXgn33x96yYAzzoCxY2HRImjRIn25SskFEh3E9srMCgBPAfucls7MupvZHDObs3LlysQXJyIikoxSUmDIkDDz/p//DAMGANCwYZj7dfZsaNtWi4TnFokOYj8CFeMeV4ht26UEUA2YZmaLgbOAsZkN2I99QSDN3dPKlSuXwJJFRESSXMGCMGxYWCz81lth0CAgPHzhBXj33TAPrBYJT37ZWmvyIMwGTjKzKoQA1hG4YtdOd18PlN312MymAX929zkJrktERCR3K1QodIG1aQPXXx8mfb36arp2/WP2/VKl4Nlnw/AySU4JDWLuvt3MbgEmASnAi+4+38weAua4+9hEPr+IiEieVqRIWBj84ovDekdFikCHDtx5Z1iX8tFHoVy5MLZfklOie8Rw9wnAhAzbHsyibcNE1yMiIpKnHHYYvPUWNGsGnTpB4cLQpg2PPBLC2N//HhYJ79kz6kIlM5EO1hcREZEcULw4jB8PdepAhw4wfjxm8NxzYeD+7bfDK69EXaRkRkFMREQkLyhRAiZOhBo1oF07eOcdUlJg+HC44AK45hoYNy7qIiUjBTEREZG84sgjw1T7p5wCrVvDtGkUKQJvvhnmGrvsMnj//aiLlHgKYiIiInlJ6dIweTJUqQItW8KsWZQoERYJr1w5jOufNy/qImUXBTEREZG8ply5EMaOOy4M4p89m7JlQ2dZyZJhKaRvvom6SAEFMRERkbzp2GNh6tTwlckLL4R586hYMYSxnTvDpp9+irpIURATERHJqypUCGGsRAlo3Bi++IJTTw1j+letCj1ja9ZEXWT+piAmIiKSl1WuHMJY4cIhjC1cSFpamHrs66/DMLLffou6yPxLQUxERCSvO/HEEMbcoVEjWLSIRo1gxAj4+GO49FLYujXqIvMnBTEREZH84NRTYcoU2LIlhLElS2jbFgYOhLffhs6dw9gxObQUxERERPKLatXCaP1ffw1h7Mcf6dYNHnsMRo6E224LnWZy6CiIiYiI5CdnnAGTJsHKlSGM/fwzd90FvXrBM8/A3/4WdYH5i4KYiIhIflO3bvjq5I8/hvWPVq7kscfCMkh/+xs8/XTUBeYfCmIiIiL5Ub16YfHJ776DCy/E1q7h+efDykg9esCwYVEXmD8oiImIiORX558PY8bAggVw0UUU/G09I0aEzV26wPjxUReY9ymIiYiI5GcXXQRvvBEWoGzWjKLbNjBmDNSsGaa1mDkz6gLzNgUxERGR/K5ly/C1yf/+Fy6+mCMKbmLiRDj++LDrf/+LusC8S0FMREREoF07GDoU3n8fWremXInNvPtuWB3pootg0aKoC8ybFMREREQkuPxyePFFePddaNeO44/ZyjvvwLZtYZHw5cujLjDvURATERGRP3TpAs8/DxMmQMeOnHbiNiZMgF9+CT1ja9dGXWDeoiAmIiIiu+veHfr3hzffhKuuom7tHYwZA199BRdfDJs2RV1g3qEgJiIiInu67TZ4/HF47TW45hoaN9rJ8OEwaxa0bx8uV8rBUxATERGRzPXqBQ8/DK+8Atdfz6Vtd/Lcc+GqZdeuWiQ8JxSMugARERFJYvffD5s3Q58+ULQo3Z9+mtWrjfvug9KlwxVMs6iLzL0UxERERGTvHn44hLEnn4QiRbjn8b6sWmU89RSULQsPPhh1gbmXgpiIiIjsnRn07QtbtsCTT2JFi9K3799ZvRr++lcoUwZuvjnqInMnBTERERHZN7NwHXLLFujThwJFi/LCC/ezdi3cemu4THn55VEXmfsoiImIiEj2FCgAzz0XwtgDD1CwSBFGjuxFs2Zw9dVQqhQ0bRp1kbmLgpiIiIhkX4ECYfb9rVvhrrs4rEgR3nrrNs4/H9q2hcmT4Zxzoi4y91AQExERkf2TkhKmtNiyBXr0oGSRIrz99vXUrw8tWoTlKqtXj7rI3EHziImIiMj+K1QIRo4MyeuGGzhqwku88w4UKxaWQvr++6gLzB0UxEREROTAFC4Mo0dDkyZwzTVUnjWcd94JM100aRLWp5S9UxATERGRA1e0KIwZA+edB1dfzelfvcGECbB8eegZW7cu6gKTm4KYiIiIHJxixeA//4Ezz4SOHTlr5Tj+/W9YsABatYLff4+6wOSlICYiIiIH7/DDwyKUtWrBpZdyEZMYOhRmzoQOHbRIeFYUxERERCRnlCwJkyZB1arQujUdjnqPZ56BceOgWzctEp4ZBTERERHJOaVKwTvvwAknQMuW3Fh9Jg8/DEOHwp13gnvUBSYXBTERERHJWeXKhZldK1SA5s35S+OP6dED+vWDRx6JurjkoiAmIiIiOe+YY2DqVChXDmt6EU9d+QlXXgn33x9WSZJAQUxEREQSo3z5EMZKlqRA0wt58fbPadkSbroJXn896uKSg4KYiIiIJE6lSiGMFS1KoWaNGfXwV9SvD1deGYaS5XcJD2Jm1tTMFprZt2Z2Tyb77zCzBWb2PzObYmaVEl2TiIiIHEInnABTpoAZRZs34j/9vqVqVWjTBj76KOriopXQIGZmKcAzQDOgKnC5mVXN0OxTIM3dawCjgccTWZOIiIhE4JRTQhjbto0jWjfi3UGLOfbYsFTl/PlRFxedRPeI1QW+dffv3H0rMBK4JL6Bu7/n7ptiDz8CKiS4JhEREYnC6afDu+/Cxo2U69CIqS8vpUgRuPBCWLw46uKikeggVh5YGvd4WWxbVroBEzPbYWbdzWyOmc1ZuXJlDpYoIiIih0xqapj0dfVqju96AVNeXc6mTSGMrVgRdXGHXtIM1jezK4E0oG9m+919oLunuXtauXLlDm1xIiIiknPq1IGJE+Gnnzjtlgt459UVLFsGTZvCr79GXdyhlegg9iNQMe5xhdi23ZhZY+AvQCt335LgmkRERCRq55wD48fD4sXUua8JY4es5vPPwyLhmzdHXdyhk+ggNhs4ycyqmFlhoCMwNr6BmdUCnieEsHzYKSkiIpJPNWgAY8fCwoU07nsRI55dx/vvQ8eOsH171MUdGgkNYu6+HbgFmAR8Cbzu7vPN7CEzaxVr1hc4HBhlZvPMbGwWpxMREZG8pnFjeOMN+N//uHRwM559fANvvQXXXZc/1qUsmOgncPcJwIQM2x6Mu9840TWIiIhIEmvRAl57Ddq35/pCLVh930T+8khxypSBvn3BLOoCEydpBuuLiIhIPtamDQwbBh98wL0fteL2G37nySfhsceiLiyxEt4jJiIiIpItHTrA1q1Y5848Wbgdqzu8yb33FqFMmXCpMi9SEBMREZHkcdVVsGULdt11DLm4A2suGsUNNxSiVCm49NKoi8t5ujQpIiIiyeXaa2HAAAqMe4s3i3ei/lnb6dQJJk+OurCcpyAmIiIiyefmm+HJJyn471G8U74Lp528g9at4b//jbqwnKUgJiIiIsnpjjugTx+KjBrGrOrXc3S5nTRvDl9+GXVhOUdBTERERJLXfffBAw9QbMRgPjnnFgqmOBdeCD/8EHVhOUNBTERERJLb3/4Gd91FyeHP8nmTO9jwawhjK1dGXdjBUxATERGR5GYGjz4Kt91GuWH9+Pzi+1iy2GnWDDZsiLq4g6MgJiIiIsnPDPr1g+uvp+KwR/ms3UPMmwetW+fuRcIVxERERCR3MIP/+z/o0oWTh/dmdrtHmToVrrgi9y4SriAmIiIiuUeBAvDCC3D55dR6/V6mt+nHm2/CDTfkzkXCNbO+iIiI5C4pKfDKK7B1K+e9cTtjmxWh1eAbKVs2DCXLTdQjJiIiIrlPwYIwfDhcfDEXT7yJoee/yGOPQd++URe2f9QjJiIiIrlT4cIwahRccgmd3rmWdWcV4da7OlG6NHTrFnVx2aMgJiIiIrlXkSLw5ptYixbcPP1q1tcsTPfu7SldGtq0ibq4fdOlSREREcndDjsMxo3DzjmH++ZfwZ0nvkXHjvDee1EXtm8KYiIiIpL7FS8O48djZ5zBY9+3p+sxE2nVCubMibqwvVMQExERkbzhiCPg7bexatV49pc2tCo+hWbNYOHCqAvLmoKYiIiI5B2lSsG772Inn8zQ9Rdzzvb3adIEli6NurDMKYiJiIhI3lKmDLz7LgUqV+LfW1tw0uqPuPBCWLUq6sL2pCAmIiIiec/RR8OUKaQcdwyTCjSl1Hdzad48+RYJVxATERGRvOm442DqVAqWLcW0wk3YPvcz2raFLVuiLuwPCmIiIiKSd1WsCFOnUvjI4nxQvAk/Tl7AlVfCjh1RFxYoiImIiEjeVqUKTJnCYcVTmF3iAuaN/oYmTaBSpbCGeOXKMGxYNKUpiImIiEjed/LJMGUKxYvu4MOijaj+Xj+m/1CZ7V6AaUsqM7nrsEjCmIKYiIiI5A9Vq8LkyRTdvJZ/cgeVWUIBnMosYcC27nzc49AnMQUxERERyT9q1GADJSiA77a5OJu4Y/VfDnk5CmIiIiKSrxzNL5luP54fDnElCmIiIiKSz2wqc/x+bU8kBTERERHJVw7v34fthYvttm174WIc3r/PIa9FQUxERETyl06dKPjiwDB/hRlUqhQed+p0yEspeMifUURERCRqnTpFErwyUo+YiIiISEQUxEREREQioiAmIiIiEhEFMREREZGIKIiJiIiIRERBTERERCQiCmIiIiIiEUl4EDOzpma20My+NbN7MtlfxMxei+3/2MwqJ7omERERkWSQ0CBmZinAM0AzoCpwuZlVzdCsG7DW3U8E/gk8lsiaRERERJJFonvE6gLfuvt37r4VGAlckqHNJcDLsfujgQvMzBJcl4iIiEjkEh3EygNL4x4vi23LtI27bwfWA2USXJeIiIhI5HLNWpNm1h3oHnu40cwWJvgpywKrEvwcIgdDn1HJDfQ5lWR3KD6jlbLakegg9iNQMe5xhdi2zNosM7OCQElgdcYTuftAYGCC6tyDmc1x97RD9Xwi+0ufUckN9DmVZBf1ZzTRlyZnAyeZWRUzKwx0BMZmaDMW6By7fykw1d09wXWJiIiIRC6hPWLuvt3MbgEmASnAi+4+38weAua4+1hgMDDUzL4F1hDCmoiIiEiel/AxYu4+AZiQYduDcfc3A+0TXccBOGSXQUUOkD6jkhvocyrJLtLPqOkqoIiIiEg0tMSRiIiISEQUxDIwsxfNbIWZfRF1LSKZMbOKZvaemS0ws/lm1iPqmkTimVlRM/uvmX0W+4z+LeqaRDJjZilm9qmZ/SeqGhTE9vQS0DTqIkT2Yjtwp7tXBc4Cbs5k6TCRKG0BGrl7TSAVaGpmZ0VbkkimegBfRlmAglgG7v4+4dubIknJ3Ze7+yex+xsIv0QyrlghEhkPNsYeFordNCBZkoqZVQBaAC9EWYeCmEguZmaVgVrAxxGXIrKb2CWfecAK4F1312dUkk0/4C5gZ5RFKIiJ5FJmdjjwBtDT3X+Nuh6ReO6+w91TCSuq1DWzahGXJJLOzFoCK9x9btS1KIiJ5EJmVogQwoa5+7+jrkckK+6+DngPjb2V5FIPaGVmi4GRQCMzezWKQhTERHIZMzPCihRfuvtTUdcjkpGZlTOzI2P3DwOaAF9FWpRIHHe/190ruHtlwoo+U939yihqURDLwMxGAB8Cp5jZMjPrFnVNIhnUA64i/AU3L3ZrHnVRInGOBd4zs/8R1hx+190jmx5AJJlpZn0RERGRiKhHTERERCQiCmIiIiIiEVEQExEREYmIgpiIiIhIRBTERERERCKiICYiuYqZtU7EIudm1tvM/ryfx2zcd6ssj+1pZsVy4lwiknspiIlIbtMayNEgZmYFc/J82dQTKLavRiKStymIiUhkzKyymX1pZoPMbL6ZvRObiR0zO8HM3jazuWY2w8xONbNzgFZA39hEtmea2dxY+5pm5mZ2fOzxIjMrFnuOqWb2PzObErf/JTN7zsw+Bh7PUNd1ZjZxVy1x26uY2Ydm9rmZ/T3Dvl5mNjv2PH+Le31fmdmw2OscHavpNuA4wqSn78Wdo4+ZfWZmH5nZ0Tn8dotIElIQE5GonQQ84+6nA+uAdrHtA4Fb3b028Gfg/9x9FjAW6OXuqe7+MVDUzI4AzgXmAOeaWSXCgr6bgH8BL7t7DWAY8HTcc1cAznH3O3ZtMLNbgJZAa3f/PUOt/YFn3b06sDzumAtjr6MukArUNrPzYrtPidV+GvArcJO7Pw38BJzv7ufH2hUHPnL3msD7wHX79S6KSK6kICYiUfve3efF7s8FKpvZ4cA5wCgzmwc8T1g2JzOzCMs+nQc8Evt5LjAjtv9sYHjs/lCgftyxo9x9R9zjq4FmwKXuviWT56oHjIg71y4Xxm6fAp8ApxKCGcBSd/8gdv/VDM8fbyuwaxmguUDlLNqJSB4SxbgIEZF48YFnB3AY4Y/Ede6emo3j3ycEr0rAW8DdgAPjs3Hsbxkef07o0aoAfJ/FMZmtC2fAP9z9+d02mlXOpH1W68pt8z/WnNuBfj+L5AvqERORpOPuvwLfm1l7AAtqxnZvAErENZ8BXAl84+47gTVAc2BmbP8soGPsfif+6CnLzKfA9cBYMzsuk/0fZDjXLpOAa2I9eZhZeTM7KrbveDM7O3b/iri6Mr4OEcmHFMREJFl1ArqZ2WfAfOCS2PaRQC8z+9TMTnD3xYQeqfdj+2cSetPWxh7fCnQ1s/8BVwE99vak7j6TMCZtvJmVzbC7B3CzmX0OlI875h3C5c8PY/tG80fIWhg75kugFPBsbPtA4O34wfoikv/YHz3hIiKSk2KXJv/j7tWirkVEkpN6xEREREQioh4xERERkYioR0xEREQkIgpiIiIiIhFREBMRERGJiIKYiIiISEQUxEREREQioiAmIiIiEpH/ByhRc+QypJBSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different depth\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(network_depth, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(network_depth, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs network depth')\n",
    "plt.xlabel('network depth')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(network_depth)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [512]\n",
      "epoch 50\n",
      "accuracy on train data:  0.701\n",
      "learning rate:  0.001414213562373095\n",
      "softmax loss:  2.4384530256167004e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7294\n",
      "learning rate:  0.001\n",
      "softmax loss:  2.2590405138106635e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7535\n",
      "learning rate:  0.0008164965809277261\n",
      "softmax loss:  2.4823077935786723e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7701\n",
      "learning rate:  0.0007071067811865475\n",
      "softmax loss:  2.5202653816610038e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.7705\n",
      "learning rate:  0.0007053456158585983\n",
      "softmax loss:  2.518159706339217e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.7705\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      2031\n",
      "           1       0.81      0.83      0.82      1937\n",
      "           2       0.65      0.70      0.68      1809\n",
      "           3       0.57      0.62      0.59      1849\n",
      "           4       0.85      0.75      0.80      2374\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      " samples avg       0.77      0.77      0.77     10000\n",
      "\n",
      "accuracy on test data:  0.769\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       232\n",
      "           1       0.80      0.81      0.81       194\n",
      "           2       0.61      0.69      0.65       176\n",
      "           3       0.63      0.59      0.61       199\n",
      "           4       0.80      0.75      0.77       199\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1000\n",
      "   macro avg       0.76      0.76      0.76      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      " samples avg       0.77      0.77      0.77      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256]\n",
      "epoch 50\n",
      "accuracy on train data:  0.6913\n",
      "learning rate:  0.001414213562373095\n",
      "softmax loss:  2.829604797009096e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7283\n",
      "learning rate:  0.001\n",
      "softmax loss:  2.5478154341155774e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7609\n",
      "learning rate:  0.0008164965809277261\n",
      "softmax loss:  3.3394030775105174e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7788\n",
      "learning rate:  0.0007071067811865475\n",
      "softmax loss:  2.511949838152347e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.779\n",
      "learning rate:  0.0007053456158585983\n",
      "softmax loss:  2.4884937458219116e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.779\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      1996\n",
      "           1       0.84      0.86      0.85      1951\n",
      "           2       0.66      0.72      0.69      1801\n",
      "           3       0.56      0.61      0.58      1850\n",
      "           4       0.85      0.74      0.79      2402\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.79      0.78      0.78     10000\n",
      " samples avg       0.78      0.78      0.78     10000\n",
      "\n",
      "accuracy on test data:  0.78\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       228\n",
      "           1       0.83      0.83      0.83       199\n",
      "           2       0.63      0.71      0.67       177\n",
      "           3       0.62      0.60      0.61       193\n",
      "           4       0.80      0.74      0.77       203\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1000\n",
      "   macro avg       0.77      0.77      0.77      1000\n",
      "weighted avg       0.78      0.78      0.78      1000\n",
      " samples avg       0.78      0.78      0.78      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128]\n",
      "epoch 50\n",
      "accuracy on train data:  0.6908\n",
      "learning rate:  0.001414213562373095\n",
      "softmax loss:  2.5470862546841987e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.734\n",
      "learning rate:  0.001\n",
      "softmax loss:  3.018135410329875e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7671\n",
      "learning rate:  0.0008164965809277261\n",
      "softmax loss:  3.57361083756996e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7839\n",
      "learning rate:  0.0007071067811865475\n",
      "softmax loss:  1.8145684474911947e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.7838\n",
      "learning rate:  0.0007053456158585983\n",
      "softmax loss:  1.784769632453523e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.7838\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1991\n",
      "           1       0.85      0.87      0.86      1923\n",
      "           2       0.70      0.72      0.71      1914\n",
      "           3       0.54      0.62      0.58      1762\n",
      "           4       0.85      0.73      0.79      2410\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.79      0.78      0.79     10000\n",
      " samples avg       0.78      0.78      0.78     10000\n",
      "\n",
      "accuracy on test data:  0.783\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       229\n",
      "           1       0.83      0.85      0.84       194\n",
      "           2       0.68      0.71      0.70       192\n",
      "           3       0.57      0.60      0.58       176\n",
      "           4       0.80      0.72      0.76       209\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1000\n",
      "   macro avg       0.77      0.77      0.77      1000\n",
      "weighted avg       0.79      0.78      0.78      1000\n",
      " samples avg       0.78      0.78      0.78      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128, 64]\n",
      "Convergence criteria satisfied!\n",
      "epoch 6\n",
      "accuracy on train data:  0.2091\n",
      "learning rate:  0.004082482904638631\n",
      "softmax loss:  0.000232068269599483\n",
      "\n",
      "\n",
      "accuracy on train data:  0.2091\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "   micro avg       0.21      0.21      0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      " samples avg       0.21      0.21      0.21     10000\n",
      "\n",
      "accuracy on test data:  0.187\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "   micro avg       0.19      0.19      0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      " samples avg       0.19      0.19      0.19      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512,256], [512,256,128], [512,256,128,64]]\n",
    "network_depth = [1,2,3,4]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    NN = NeuralNetwork(n = 1024, n_hidden_nodes = hidden_layer , r = 5, M = 32)\n",
    "    NN.train(X_train, y_train_onehot, epoch_mode= False, activation=\"sigmoid\", adaptive_learning = True, alpha = 0.01, stopping_threshold = 5.0e-06, printafter=50)\n",
    "    y_pred_train, _ = NN.predict(X_train)\n",
    "    y_pred_test, _ = NN.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different depth\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(network_depth, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(network_depth, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs network depth')\n",
    "plt.xlabel('network depth')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(network_depth)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [512]\n",
      "Convergence criteria satisfied!\n",
      "epoch 6\n",
      "accuracy on train data:  0.2091\n",
      "learning rate:  0.004082482904638631\n",
      "softmax loss:  0.00023208783009197323\n",
      "\n",
      "\n",
      "accuracy on train data:  0.2091\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "   micro avg       0.21      0.21      0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      " samples avg       0.21      0.21      0.21     10000\n",
      "\n",
      "accuracy on test data:  0.187\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "   micro avg       0.19      0.19      0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      " samples avg       0.19      0.19      0.19      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence criteria satisfied!\n",
      "epoch 6\n",
      "accuracy on train data:  0.2091\n",
      "learning rate:  0.004082482904638631\n",
      "softmax loss:  0.00023044619233218206\n",
      "\n",
      "\n",
      "accuracy on train data:  0.2091\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.21      0.35     10000\n",
      "\n",
      "   micro avg       0.21      0.21      0.21     10000\n",
      "   macro avg       0.20      0.04      0.07     10000\n",
      "weighted avg       1.00      0.21      0.35     10000\n",
      " samples avg       0.21      0.21      0.21     10000\n",
      "\n",
      "accuracy on test data:  0.187\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       1.00      0.19      0.32      1000\n",
      "\n",
      "   micro avg       0.19      0.19      0.19      1000\n",
      "   macro avg       0.20      0.04      0.06      1000\n",
      "weighted avg       1.00      0.19      0.32      1000\n",
      " samples avg       0.19      0.19      0.19      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50\n",
      "accuracy on train data:  0.7289\n",
      "learning rate:  0.001414213562373095\n",
      "softmax loss:  3.145936113878991e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7583\n",
      "learning rate:  0.001\n",
      "softmax loss:  3.272657747861718e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7812\n",
      "learning rate:  0.0008164965809277261\n",
      "softmax loss:  1.1503443467968143e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7962\n",
      "learning rate:  0.0007071067811865475\n",
      "softmax loss:  1.7943773712320744e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8045\n",
      "learning rate:  0.0007053456158585983\n",
      "softmax loss:  2.1482919875104044e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8045\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1966\n",
      "           1       0.87      0.93      0.90      1845\n",
      "           2       0.66      0.80      0.72      1624\n",
      "           3       0.59      0.61      0.60      1932\n",
      "           4       0.91      0.72      0.80      2633\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     10000\n",
      "   macro avg       0.80      0.81      0.80     10000\n",
      "weighted avg       0.82      0.80      0.81     10000\n",
      " samples avg       0.80      0.80      0.80     10000\n",
      "\n",
      "accuracy on test data:  0.762\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       226\n",
      "           1       0.80      0.88      0.84       179\n",
      "           2       0.57      0.70      0.63       164\n",
      "           3       0.57      0.54      0.55       199\n",
      "           4       0.85      0.69      0.76       232\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1000\n",
      "   macro avg       0.75      0.76      0.75      1000\n",
      "weighted avg       0.77      0.76      0.76      1000\n",
      " samples avg       0.76      0.76      0.76      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128, 64]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7216\n",
      "learning rate:  0.001414213562373095\n",
      "softmax loss:  1.9969842319606146e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7709\n",
      "learning rate:  0.001\n",
      "softmax loss:  1.154033885296016e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8061\n",
      "learning rate:  0.0008164965809277261\n",
      "softmax loss:  9.192962043513032e-07\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8028\n",
      "learning rate:  0.0007071067811865475\n",
      "softmax loss:  4.717764805687347e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.832\n",
      "learning rate:  0.0007053456158585983\n",
      "softmax loss:  9.181468735261794e-07\n",
      "\n",
      "\n",
      "accuracy on train data:  0.832\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1988\n",
      "           1       0.93      0.91      0.92      2034\n",
      "           2       0.79      0.80      0.80      1932\n",
      "           3       0.57      0.71      0.63      1602\n",
      "           4       0.88      0.76      0.81      2444\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.85      0.83      0.84     10000\n",
      " samples avg       0.83      0.83      0.83     10000\n",
      "\n",
      "accuracy on test data:  0.761\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       225\n",
      "           1       0.86      0.81      0.83       210\n",
      "           2       0.62      0.68      0.65       182\n",
      "           3       0.52      0.56      0.54       174\n",
      "           4       0.80      0.71      0.75       209\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1000\n",
      "   macro avg       0.75      0.75      0.75      1000\n",
      "weighted avg       0.77      0.76      0.76      1000\n",
      " samples avg       0.76      0.76      0.76      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512,256], [512,256,128], [512,256,128,64]]\n",
    "network_depth = [1,2,3,4]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    NN = NeuralNetwork(n = 1024, n_hidden_nodes = hidden_layer , r = 5, M = 32)\n",
    "    NN.train(X_train, y_train_onehot, epoch_mode= False, activation=\"relu\", adaptive_learning = True, alpha = 0.01, stopping_threshold = 5.0e-06, printafter=50)\n",
    "    y_pred_train, _ = NN.predict(X_train, activation=\"relu\")\n",
    "    y_pred_test, _ = NN.predict(X_test, activation=\"relu\")\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different depth\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(network_depth, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(network_depth, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs network depth')\n",
    "plt.xlabel('network depth')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(network_depth)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relu activation function (with learning rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [512]\n",
      "epoch 50\n",
      "accuracy on train data:  0.6984\n",
      "learning rate:  0.0001414213562373095\n",
      "softmax loss:  3.2206525277455114e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7177\n",
      "learning rate:  0.0001\n",
      "softmax loss:  2.8674372883269804e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.7355\n",
      "learning rate:  8.164965809277261e-05\n",
      "softmax loss:  2.6139527954960384e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.7488\n",
      "learning rate:  7.071067811865475e-05\n",
      "softmax loss:  2.3554841256167726e-05\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.7493\n",
      "learning rate:  7.053456158585983e-05\n",
      "softmax loss:  2.3523313554780805e-05\n",
      "\n",
      "\n",
      "accuracy on train data:  0.7493\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      2050\n",
      "           1       0.78      0.77      0.78      1990\n",
      "           2       0.64      0.66      0.65      1904\n",
      "           3       0.54      0.62      0.58      1748\n",
      "           4       0.84      0.76      0.80      2308\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     10000\n",
      "   macro avg       0.75      0.74      0.75     10000\n",
      "weighted avg       0.76      0.75      0.75     10000\n",
      " samples avg       0.75      0.75      0.75     10000\n",
      "\n",
      "accuracy on test data:  0.734\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       228\n",
      "           1       0.77      0.76      0.76       200\n",
      "           2       0.60      0.63      0.61       189\n",
      "           3       0.57      0.56      0.56       189\n",
      "           4       0.75      0.73      0.74       194\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1000\n",
      "   macro avg       0.73      0.72      0.73      1000\n",
      "weighted avg       0.74      0.73      0.73      1000\n",
      " samples avg       0.73      0.73      0.73      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7485\n",
      "learning rate:  0.0001414213562373095\n",
      "softmax loss:  2.3803159130456424e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.7995\n",
      "learning rate:  0.0001\n",
      "softmax loss:  1.7794089613993414e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8238\n",
      "learning rate:  8.164965809277261e-05\n",
      "softmax loss:  1.1009632399852487e-05\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.842\n",
      "learning rate:  7.071067811865475e-05\n",
      "softmax loss:  8.309603697956972e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8419\n",
      "learning rate:  7.053456158585983e-05\n",
      "softmax loss:  8.155999959208848e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8419\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1956\n",
      "           1       0.88      0.91      0.90      1910\n",
      "           2       0.77      0.81      0.78      1857\n",
      "           3       0.69      0.71      0.70      1963\n",
      "           4       0.89      0.80      0.84      2314\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      " samples avg       0.84      0.84      0.84     10000\n",
      "\n",
      "accuracy on test data:  0.803\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       224\n",
      "           1       0.85      0.86      0.86       196\n",
      "           2       0.69      0.75      0.72       182\n",
      "           3       0.64      0.62      0.63       191\n",
      "           4       0.83      0.75      0.79       207\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1000\n",
      "   macro avg       0.80      0.80      0.80      1000\n",
      "weighted avg       0.80      0.80      0.80      1000\n",
      " samples avg       0.80      0.80      0.80      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7774\n",
      "learning rate:  0.0001414213562373095\n",
      "softmax loss:  2.9403732478028548e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.8168\n",
      "learning rate:  0.0001\n",
      "softmax loss:  1.2094483311416297e-05\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.847\n",
      "learning rate:  8.164965809277261e-05\n",
      "softmax loss:  8.964043796841776e-06\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8617\n",
      "learning rate:  7.071067811865475e-05\n",
      "softmax loss:  5.319551939104827e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8618\n",
      "learning rate:  7.053456158585983e-05\n",
      "softmax loss:  5.11851412273001e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8618\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1922\n",
      "           1       0.88      0.93      0.91      1872\n",
      "           2       0.82      0.83      0.82      1927\n",
      "           3       0.71      0.76      0.74      1893\n",
      "           4       0.92      0.81      0.86      2386\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      " samples avg       0.86      0.86      0.86     10000\n",
      "\n",
      "accuracy on test data:  0.816\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       224\n",
      "           1       0.83      0.89      0.86       186\n",
      "           2       0.70      0.76      0.73       182\n",
      "           3       0.72      0.64      0.67       211\n",
      "           4       0.83      0.79      0.81       197\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.81      0.82      0.81      1000\n",
      " samples avg       0.82      0.82      0.82      1000\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128, 64]\n",
      "epoch 50\n",
      "accuracy on train data:  0.7796\n",
      "learning rate:  0.0001414213562373095\n",
      "softmax loss:  2.1823853735152896e-05\n",
      "\n",
      "\n",
      "epoch 100\n",
      "accuracy on train data:  0.8303\n",
      "learning rate:  0.0001\n",
      "softmax loss:  9.080570686102212e-06\n",
      "\n",
      "\n",
      "epoch 150\n",
      "accuracy on train data:  0.8567\n",
      "learning rate:  8.164965809277261e-05\n",
      "softmax loss:  5.53839396842743e-06\n",
      "\n",
      "\n",
      "epoch 200\n",
      "accuracy on train data:  0.8847\n",
      "learning rate:  7.071067811865475e-05\n",
      "softmax loss:  5.457597634399534e-06\n",
      "\n",
      "\n",
      "Convergence criteria satisfied!\n",
      "epoch 201\n",
      "accuracy on train data:  0.8908\n",
      "learning rate:  7.053456158585983e-05\n",
      "softmax loss:  3.781473057701032e-06\n",
      "\n",
      "\n",
      "accuracy on train data:  0.8908\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1990\n",
      "           1       0.93      0.95      0.94      1941\n",
      "           2       0.85      0.87      0.86      1903\n",
      "           3       0.74      0.81      0.78      1832\n",
      "           4       0.94      0.84      0.89      2334\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      " samples avg       0.89      0.89      0.89     10000\n",
      "\n",
      "accuracy on test data:  0.82\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       232\n",
      "           1       0.87      0.91      0.89       191\n",
      "           2       0.71      0.80      0.76       177\n",
      "           3       0.67      0.63      0.65       201\n",
      "           4       0.81      0.76      0.78       199\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1000\n",
      "   macro avg       0.81      0.82      0.81      1000\n",
      "weighted avg       0.82      0.82      0.82      1000\n",
      " samples avg       0.82      0.82      0.82      1000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512,256], [512,256,128], [512,256,128,64]]\n",
    "network_depth = [1,2,3,4]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    NN = NeuralNetwork(n = 1024, n_hidden_nodes = hidden_layer , r = 5, M = 32)\n",
    "    NN.train(X_train, y_train_onehot, epoch_mode= False, activation=\"relu\", adaptive_learning = True, alpha = 0.001, stopping_threshold = 5.0e-06, printafter=50)\n",
    "    y_pred_train, _ = NN.predict(X_train, activation=\"relu\")\n",
    "    y_pred_test, _ = NN.predict(X_test, activation=\"relu\")\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different depth\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(network_depth, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(network_depth, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs network depth')\n",
    "plt.xlabel('network depth')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(network_depth)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Networks using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [512]\n",
      "Iteration 1, loss = 1.82100347\n",
      "Iteration 2, loss = 1.71391782\n",
      "Iteration 3, loss = 1.52315981\n",
      "Iteration 4, loss = 1.51474497\n",
      "Iteration 5, loss = 1.51029548\n",
      "Iteration 6, loss = 1.50750493\n",
      "Iteration 7, loss = 1.50565037\n",
      "Iteration 8, loss = 1.50367111\n",
      "Iteration 9, loss = 1.50203666\n",
      "Iteration 10, loss = 1.50045418\n",
      "Iteration 11, loss = 1.49933494\n",
      "Iteration 12, loss = 1.49815999\n",
      "Iteration 13, loss = 1.49698195\n",
      "Iteration 14, loss = 1.49620743\n",
      "Iteration 15, loss = 1.49550628\n",
      "Iteration 16, loss = 1.49454919\n",
      "Iteration 17, loss = 1.49363666\n",
      "Iteration 18, loss = 1.49279152\n",
      "Iteration 19, loss = 1.49200207\n",
      "Iteration 20, loss = 1.49143824\n",
      "Iteration 21, loss = 1.49060290\n",
      "Iteration 22, loss = 1.48995872\n",
      "Iteration 23, loss = 1.48938653\n",
      "Iteration 24, loss = 1.48872835\n",
      "Iteration 25, loss = 1.48810307\n",
      "Iteration 26, loss = 1.48745704\n",
      "Iteration 27, loss = 1.48675391\n",
      "Iteration 28, loss = 1.48601222\n",
      "Iteration 29, loss = 1.48555051\n",
      "Iteration 30, loss = 1.48489258\n",
      "Iteration 31, loss = 1.48442929\n",
      "Iteration 32, loss = 1.48394169\n",
      "Iteration 33, loss = 1.48350804\n",
      "Iteration 34, loss = 1.48292553\n",
      "Iteration 35, loss = 1.48245257\n",
      "Iteration 36, loss = 1.48207413\n",
      "Iteration 37, loss = 1.48157765\n",
      "Iteration 38, loss = 1.48119172\n",
      "Iteration 39, loss = 1.48078643\n",
      "Iteration 40, loss = 1.48028934\n",
      "Iteration 41, loss = 1.47994385\n",
      "Iteration 42, loss = 1.47956204\n",
      "Iteration 43, loss = 1.47913680\n",
      "Iteration 44, loss = 1.47881800\n",
      "Iteration 45, loss = 1.47835719\n",
      "Iteration 46, loss = 1.47806096\n",
      "Iteration 47, loss = 1.47762217\n",
      "Iteration 48, loss = 1.47733798\n",
      "Iteration 49, loss = 1.47684794\n",
      "Iteration 50, loss = 1.47658435\n",
      "Iteration 51, loss = 1.47617737\n",
      "Iteration 52, loss = 1.47584291\n",
      "Iteration 53, loss = 1.47550371\n",
      "Iteration 54, loss = 1.47513914\n",
      "Iteration 55, loss = 1.47476276\n",
      "Iteration 56, loss = 1.47438837\n",
      "Iteration 57, loss = 1.47401940\n",
      "Iteration 58, loss = 1.47368000\n",
      "Iteration 59, loss = 1.47319879\n",
      "Iteration 60, loss = 1.47285898\n",
      "Iteration 61, loss = 1.47256885\n",
      "Iteration 62, loss = 1.47207112\n",
      "Iteration 63, loss = 1.47172472\n",
      "Iteration 64, loss = 1.47138364\n",
      "Iteration 65, loss = 1.47103137\n",
      "Iteration 66, loss = 1.47073728\n",
      "Iteration 67, loss = 1.47036312\n",
      "Iteration 68, loss = 1.47003571\n",
      "Iteration 69, loss = 1.46979185\n",
      "Iteration 70, loss = 1.46951244\n",
      "Iteration 71, loss = 1.46925222\n",
      "Iteration 72, loss = 1.46899571\n",
      "Iteration 73, loss = 1.46865432\n",
      "Iteration 74, loss = 1.46840627\n",
      "Iteration 75, loss = 1.46819481\n",
      "Iteration 76, loss = 1.46778235\n",
      "Iteration 77, loss = 1.46766729\n",
      "Iteration 78, loss = 1.46732930\n",
      "Iteration 79, loss = 1.46709043\n",
      "Iteration 80, loss = 1.46691234\n",
      "Iteration 81, loss = 1.46659407\n",
      "Iteration 82, loss = 1.46647751\n",
      "Iteration 83, loss = 1.46619232\n",
      "Iteration 84, loss = 1.46591034\n",
      "Iteration 85, loss = 1.46570945\n",
      "Iteration 86, loss = 1.46540203\n",
      "Iteration 87, loss = 1.46522251\n",
      "Iteration 88, loss = 1.46499896\n",
      "Iteration 89, loss = 1.46479823\n",
      "Iteration 90, loss = 1.46448519\n",
      "Iteration 91, loss = 1.46429983\n",
      "Iteration 92, loss = 1.46408573\n",
      "Iteration 93, loss = 1.46382916\n",
      "Iteration 94, loss = 1.46363847\n",
      "Iteration 95, loss = 1.46335830\n",
      "Iteration 96, loss = 1.46317960\n",
      "Iteration 97, loss = 1.46292187\n",
      "Iteration 98, loss = 1.46274574\n",
      "Iteration 99, loss = 1.46254516\n",
      "Iteration 100, loss = 1.46228647\n",
      "Iteration 101, loss = 1.46209452\n",
      "Iteration 102, loss = 1.46192999\n",
      "Iteration 103, loss = 1.46172054\n",
      "Iteration 104, loss = 1.46138636\n",
      "Iteration 105, loss = 1.46131177\n",
      "Iteration 106, loss = 1.46108355\n",
      "Iteration 107, loss = 1.46087098\n",
      "Iteration 108, loss = 1.46064470\n",
      "Iteration 109, loss = 1.46041485\n",
      "Iteration 110, loss = 1.46027431\n",
      "Iteration 111, loss = 1.46008784\n",
      "Iteration 112, loss = 1.45988650\n",
      "Iteration 113, loss = 1.45972962\n",
      "Iteration 114, loss = 1.45952104\n",
      "Iteration 115, loss = 1.45931163\n",
      "Iteration 116, loss = 1.45909816\n",
      "Iteration 117, loss = 1.45895664\n",
      "Iteration 118, loss = 1.45877721\n",
      "Iteration 119, loss = 1.45861600\n",
      "Iteration 120, loss = 1.45835821\n",
      "Iteration 121, loss = 1.45818034\n",
      "Iteration 122, loss = 1.45801117\n",
      "Iteration 123, loss = 1.45784181\n",
      "Iteration 124, loss = 1.45764247\n",
      "Iteration 125, loss = 1.45748175\n",
      "Iteration 126, loss = 1.45728778\n",
      "Iteration 127, loss = 1.45711550\n",
      "Iteration 128, loss = 1.45688891\n",
      "Iteration 129, loss = 1.45679229\n",
      "Iteration 130, loss = 1.45655648\n",
      "Iteration 131, loss = 1.45639470\n",
      "Iteration 132, loss = 1.45618799\n",
      "Iteration 133, loss = 1.45606132\n",
      "Iteration 134, loss = 1.45590004\n",
      "Iteration 135, loss = 1.45568361\n",
      "Iteration 136, loss = 1.45551304\n",
      "Iteration 137, loss = 1.45533163\n",
      "Iteration 138, loss = 1.45524969\n",
      "Iteration 139, loss = 1.45502739\n",
      "Iteration 140, loss = 1.45485948\n",
      "Iteration 141, loss = 1.45471283\n",
      "Iteration 142, loss = 1.45452669\n",
      "Iteration 143, loss = 1.45431788\n",
      "Iteration 144, loss = 1.45420511\n",
      "Iteration 145, loss = 1.45401596\n",
      "Iteration 146, loss = 1.45389316\n",
      "Iteration 147, loss = 1.45370787\n",
      "Iteration 148, loss = 1.45356289\n",
      "Iteration 149, loss = 1.45343819\n",
      "Iteration 150, loss = 1.45324188\n",
      "Iteration 151, loss = 1.45308210\n",
      "Iteration 152, loss = 1.45292444\n",
      "Iteration 153, loss = 1.45276024\n",
      "Iteration 154, loss = 1.45260947\n",
      "Iteration 155, loss = 1.45243541\n",
      "Iteration 156, loss = 1.45227674\n",
      "Iteration 157, loss = 1.45219223\n",
      "Iteration 158, loss = 1.45195802\n",
      "Iteration 159, loss = 1.45185917\n",
      "Iteration 160, loss = 1.45166933\n",
      "Iteration 161, loss = 1.45155882\n",
      "Iteration 162, loss = 1.45138742\n",
      "Iteration 163, loss = 1.45124925\n",
      "Iteration 164, loss = 1.45108788\n",
      "Iteration 165, loss = 1.45094660\n",
      "Iteration 166, loss = 1.45077949\n",
      "Iteration 167, loss = 1.45066910\n",
      "Iteration 168, loss = 1.45050262\n",
      "Iteration 169, loss = 1.45032493\n",
      "Iteration 170, loss = 1.45023156\n",
      "Iteration 171, loss = 1.45006452\n",
      "Iteration 172, loss = 1.44994194\n",
      "Iteration 173, loss = 1.44978832\n",
      "Iteration 174, loss = 1.44964848\n",
      "Iteration 175, loss = 1.44951194\n",
      "Iteration 176, loss = 1.44936269\n",
      "Iteration 177, loss = 1.44922681\n",
      "Iteration 178, loss = 1.44907058\n",
      "Iteration 179, loss = 1.44894591\n",
      "Iteration 180, loss = 1.44877779\n",
      "Iteration 181, loss = 1.44865781\n",
      "Iteration 182, loss = 1.44852393\n",
      "Iteration 183, loss = 1.44839336\n",
      "Iteration 184, loss = 1.44824172\n",
      "Iteration 185, loss = 1.44808943\n",
      "Iteration 186, loss = 1.44795043\n",
      "Iteration 187, loss = 1.44782687\n",
      "Iteration 188, loss = 1.44768827\n",
      "Iteration 189, loss = 1.44756264\n",
      "Iteration 190, loss = 1.44745318\n",
      "Iteration 191, loss = 1.44731446\n",
      "Iteration 192, loss = 1.44718501\n",
      "Iteration 193, loss = 1.44703131\n",
      "Iteration 194, loss = 1.44688879\n",
      "Iteration 195, loss = 1.44674095\n",
      "Iteration 196, loss = 1.44659639\n",
      "Iteration 197, loss = 1.44651981\n",
      "Iteration 198, loss = 1.44630971\n",
      "Iteration 199, loss = 1.44626250\n",
      "Iteration 200, loss = 1.44608502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.3589\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      1990\n",
      "           1       0.21      0.75      0.33       557\n",
      "           2       0.03      0.58      0.07       117\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.66      0.73      0.69      1890\n",
      "\n",
      "   micro avg       0.36      0.79      0.49      4554\n",
      "   macro avg       0.36      0.59      0.39      4554\n",
      "weighted avg       0.68      0.79      0.71      4554\n",
      " samples avg       0.36      0.36      0.36      4554\n",
      "\n",
      "accuracy on test data:  0.363\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       226\n",
      "           1       0.20      0.71      0.31        56\n",
      "           2       0.05      0.60      0.08        15\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.60      0.69      0.65       163\n",
      "\n",
      "   micro avg       0.36      0.79      0.50       460\n",
      "   macro avg       0.35      0.58      0.39       460\n",
      "weighted avg       0.67      0.79      0.71       460\n",
      " samples avg       0.36      0.36      0.36       460\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75599291\n",
      "Iteration 2, loss = 1.50284355\n",
      "Iteration 3, loss = 1.39129507\n",
      "Iteration 4, loss = 1.38382212\n",
      "Iteration 5, loss = 1.37945388\n",
      "Iteration 6, loss = 1.37674041\n",
      "Iteration 7, loss = 1.37485859\n",
      "Iteration 8, loss = 1.37307523\n",
      "Iteration 9, loss = 1.37166307\n",
      "Iteration 10, loss = 1.37024469\n",
      "Iteration 11, loss = 1.36924451\n",
      "Iteration 12, loss = 1.36834121\n",
      "Iteration 13, loss = 1.36742281\n",
      "Iteration 14, loss = 1.36596267\n",
      "Iteration 15, loss = 1.36591150\n",
      "Iteration 16, loss = 1.36515085\n",
      "Iteration 17, loss = 1.36496215\n",
      "Iteration 18, loss = 1.36400063\n",
      "Iteration 19, loss = 1.36335653\n",
      "Iteration 20, loss = 1.36291710\n",
      "Iteration 21, loss = 1.36243440\n",
      "Iteration 22, loss = 1.36167984\n",
      "Iteration 23, loss = 1.36167079\n",
      "Iteration 24, loss = 1.36108227\n",
      "Iteration 25, loss = 1.36048485\n",
      "Iteration 26, loss = 1.35995516\n",
      "Iteration 27, loss = 1.35961524\n",
      "Iteration 28, loss = 1.35919002\n",
      "Iteration 29, loss = 1.35920701\n",
      "Iteration 30, loss = 1.35863334\n",
      "Iteration 31, loss = 1.35818692\n",
      "Iteration 32, loss = 1.35783203\n",
      "Iteration 33, loss = 1.35752371\n",
      "Iteration 34, loss = 1.35744524\n",
      "Iteration 35, loss = 1.35693297\n",
      "Iteration 36, loss = 1.35671225\n",
      "Iteration 37, loss = 1.35632659\n",
      "Iteration 38, loss = 1.35607923\n",
      "Iteration 39, loss = 1.35565661\n",
      "Iteration 40, loss = 1.35532491\n",
      "Iteration 41, loss = 1.35524164\n",
      "Iteration 42, loss = 1.35503213\n",
      "Iteration 43, loss = 1.35473587\n",
      "Iteration 44, loss = 1.35456664\n",
      "Iteration 45, loss = 1.35412737\n",
      "Iteration 46, loss = 1.35385597\n",
      "Iteration 47, loss = 1.35384404\n",
      "Iteration 48, loss = 1.35366097\n",
      "Iteration 49, loss = 1.35333122\n",
      "Iteration 50, loss = 1.35296687\n",
      "Iteration 51, loss = 1.35278991\n",
      "Iteration 52, loss = 1.35252206\n",
      "Iteration 53, loss = 1.35240748\n",
      "Iteration 54, loss = 1.35215867\n",
      "Iteration 55, loss = 1.35190118\n",
      "Iteration 56, loss = 1.35157876\n",
      "Iteration 57, loss = 1.35146040\n",
      "Iteration 58, loss = 1.35123069\n",
      "Iteration 59, loss = 1.35098145\n",
      "Iteration 60, loss = 1.35093349\n",
      "Iteration 61, loss = 1.35068915\n",
      "Iteration 62, loss = 1.35031236\n",
      "Iteration 63, loss = 1.35013024\n",
      "Iteration 64, loss = 1.35012739\n",
      "Iteration 65, loss = 1.35003691\n",
      "Iteration 66, loss = 1.34982555\n",
      "Iteration 67, loss = 1.34947631\n",
      "Iteration 68, loss = 1.34929738\n",
      "Iteration 69, loss = 1.34929669\n",
      "Iteration 70, loss = 1.34913114\n",
      "Iteration 71, loss = 1.34883042\n",
      "Iteration 72, loss = 1.34868134\n",
      "Iteration 73, loss = 1.34864712\n",
      "Iteration 74, loss = 1.34836011\n",
      "Iteration 75, loss = 1.34807917\n",
      "Iteration 76, loss = 1.34797559\n",
      "Iteration 77, loss = 1.34787465\n",
      "Iteration 78, loss = 1.34777258\n",
      "Iteration 79, loss = 1.34758600\n",
      "Iteration 80, loss = 1.34750898\n",
      "Iteration 81, loss = 1.34723970\n",
      "Iteration 82, loss = 1.34714625\n",
      "Iteration 83, loss = 1.34700567\n",
      "Iteration 84, loss = 1.34683882\n",
      "Iteration 85, loss = 1.34662978\n",
      "Iteration 86, loss = 1.34655793\n",
      "Iteration 87, loss = 1.34639897\n",
      "Iteration 88, loss = 1.34631632\n",
      "Iteration 89, loss = 1.34605786\n",
      "Iteration 90, loss = 1.34599202\n",
      "Iteration 91, loss = 1.34590211\n",
      "Iteration 92, loss = 1.34566993\n",
      "Iteration 93, loss = 1.34562009\n",
      "Iteration 94, loss = 1.34530316\n",
      "Iteration 95, loss = 1.34535392\n",
      "Iteration 96, loss = 1.34511840\n",
      "Iteration 97, loss = 1.34508257\n",
      "Iteration 98, loss = 1.34492088\n",
      "Iteration 99, loss = 1.34477977\n",
      "Iteration 100, loss = 1.34469043\n",
      "Iteration 101, loss = 1.34450016\n",
      "Iteration 102, loss = 1.34451317\n",
      "Iteration 103, loss = 1.34436293\n",
      "Iteration 104, loss = 1.34416883\n",
      "Iteration 105, loss = 1.34412784\n",
      "Iteration 106, loss = 1.34393020\n",
      "Iteration 107, loss = 1.34386049\n",
      "Iteration 108, loss = 1.34368240\n",
      "Iteration 109, loss = 1.34351548\n",
      "Iteration 110, loss = 1.34346316\n",
      "Iteration 111, loss = 1.34343423\n",
      "Iteration 112, loss = 1.34330118\n",
      "Iteration 113, loss = 1.34307329\n",
      "Iteration 114, loss = 1.34295323\n",
      "Iteration 115, loss = 1.34266210\n",
      "Iteration 116, loss = 1.34288639\n",
      "Iteration 117, loss = 1.34263167\n",
      "Iteration 118, loss = 1.34253032\n",
      "Iteration 119, loss = 1.34240821\n",
      "Iteration 120, loss = 1.34237846\n",
      "Iteration 121, loss = 1.34209611\n",
      "Iteration 122, loss = 1.34211200\n",
      "Iteration 123, loss = 1.34199165\n",
      "Iteration 124, loss = 1.34185707\n",
      "Iteration 125, loss = 1.34172529\n",
      "Iteration 126, loss = 1.34168345\n",
      "Iteration 127, loss = 1.34150643\n",
      "Iteration 128, loss = 1.34146673\n",
      "Iteration 129, loss = 1.34139252\n",
      "Iteration 130, loss = 1.34123276\n",
      "Iteration 131, loss = 1.34114062\n",
      "Iteration 132, loss = 1.34104560\n",
      "Iteration 133, loss = 1.34099703\n",
      "Iteration 134, loss = 1.34089745\n",
      "Iteration 135, loss = 1.34064735\n",
      "Iteration 136, loss = 1.34067901\n",
      "Iteration 137, loss = 1.34052816\n",
      "Iteration 138, loss = 1.34047595\n",
      "Iteration 139, loss = 1.34022566\n",
      "Iteration 140, loss = 1.34035068\n",
      "Iteration 141, loss = 1.34006524\n",
      "Iteration 142, loss = 1.34015107\n",
      "Iteration 143, loss = 1.34002622\n",
      "Iteration 144, loss = 1.33980957\n",
      "Iteration 145, loss = 1.33978189\n",
      "Iteration 146, loss = 1.33974959\n",
      "Iteration 147, loss = 1.33956550\n",
      "Iteration 148, loss = 1.33959023\n",
      "Iteration 149, loss = 1.33943153\n",
      "Iteration 150, loss = 1.33934654\n",
      "Iteration 151, loss = 1.33923359\n",
      "Iteration 152, loss = 1.33923656\n",
      "Iteration 153, loss = 1.33907638\n",
      "Iteration 154, loss = 1.33897517\n",
      "Iteration 155, loss = 1.33882380\n",
      "Iteration 156, loss = 1.33879743\n",
      "Iteration 157, loss = 1.33873184\n",
      "Iteration 158, loss = 1.33852695\n",
      "Iteration 159, loss = 1.33854091\n",
      "Iteration 160, loss = 1.33844815\n",
      "Iteration 161, loss = 1.33841766\n",
      "Iteration 162, loss = 1.33822323\n",
      "Iteration 163, loss = 1.33813612\n",
      "Iteration 164, loss = 1.33808318\n",
      "Iteration 165, loss = 1.33807003\n",
      "Iteration 166, loss = 1.33790580\n",
      "Iteration 167, loss = 1.33785206\n",
      "Iteration 168, loss = 1.33775579\n",
      "Iteration 169, loss = 1.33763462\n",
      "Iteration 170, loss = 1.33757735\n",
      "Iteration 171, loss = 1.33748661\n",
      "Iteration 172, loss = 1.33754250\n",
      "Iteration 173, loss = 1.33731174\n",
      "Iteration 174, loss = 1.33731091\n",
      "Iteration 175, loss = 1.33705106\n",
      "Iteration 176, loss = 1.33725832\n",
      "Iteration 177, loss = 1.33692821\n",
      "Iteration 178, loss = 1.33708370\n",
      "Iteration 179, loss = 1.33692998\n",
      "Iteration 180, loss = 1.33672309\n",
      "Iteration 181, loss = 1.33667763\n",
      "Iteration 182, loss = 1.33663205\n",
      "Iteration 183, loss = 1.33659152\n",
      "Iteration 184, loss = 1.33656869\n",
      "Iteration 185, loss = 1.33644886\n",
      "Iteration 186, loss = 1.33631704\n",
      "Iteration 187, loss = 1.33624661\n",
      "Iteration 188, loss = 1.33621864\n",
      "Iteration 189, loss = 1.33608505\n",
      "Iteration 190, loss = 1.33600338\n",
      "Iteration 191, loss = 1.33588587\n",
      "Iteration 192, loss = 1.33582098\n",
      "Iteration 193, loss = 1.33584034\n",
      "Iteration 194, loss = 1.33574212\n",
      "Iteration 195, loss = 1.33565059\n",
      "Iteration 196, loss = 1.33556211\n",
      "Iteration 197, loss = 1.33560856\n",
      "Iteration 198, loss = 1.33546104\n",
      "Iteration 199, loss = 1.33536024\n",
      "Iteration 200, loss = 1.33527121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.5267\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      2008\n",
      "           1       0.64      0.70      0.67      1806\n",
      "           2       0.35      0.59      0.44      1168\n",
      "           3       0.06      0.56      0.11       227\n",
      "           4       0.68      0.73      0.70      1967\n",
      "\n",
      "   micro avg       0.53      0.73      0.61      7176\n",
      "   macro avg       0.53      0.69      0.56      7176\n",
      "weighted avg       0.66      0.73      0.68      7176\n",
      " samples avg       0.53      0.53      0.53      7176\n",
      "\n",
      "accuracy on test data:  0.521\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       229\n",
      "           1       0.62      0.69      0.65       178\n",
      "           2       0.34      0.61      0.44       112\n",
      "           3       0.06      0.50      0.11        22\n",
      "           4       0.61      0.70      0.65       164\n",
      "\n",
      "   micro avg       0.52      0.74      0.61       705\n",
      "   macro avg       0.51      0.68      0.55       705\n",
      "weighted avg       0.65      0.74      0.68       705\n",
      " samples avg       0.52      0.52      0.52       705\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76380936\n",
      "Iteration 2, loss = 1.38822441\n",
      "Iteration 3, loss = 1.37395561\n",
      "Iteration 4, loss = 1.36949873\n",
      "Iteration 5, loss = 1.36609622\n",
      "Iteration 6, loss = 1.36384879\n",
      "Iteration 7, loss = 1.36170042\n",
      "Iteration 8, loss = 1.36104174\n",
      "Iteration 9, loss = 1.35993561\n",
      "Iteration 10, loss = 1.35869727\n",
      "Iteration 11, loss = 1.35788140\n",
      "Iteration 12, loss = 1.35715549\n",
      "Iteration 13, loss = 1.35655835\n",
      "Iteration 14, loss = 1.35555626\n",
      "Iteration 15, loss = 1.35546035\n",
      "Iteration 16, loss = 1.35481342\n",
      "Iteration 17, loss = 1.35427231\n",
      "Iteration 18, loss = 1.35355794\n",
      "Iteration 19, loss = 1.35328877\n",
      "Iteration 20, loss = 1.35279381\n",
      "Iteration 21, loss = 1.35206872\n",
      "Iteration 22, loss = 1.35187235\n",
      "Iteration 23, loss = 1.35125906\n",
      "Iteration 24, loss = 1.35109893\n",
      "Iteration 25, loss = 1.35066431\n",
      "Iteration 26, loss = 1.35031724\n",
      "Iteration 27, loss = 1.34988212\n",
      "Iteration 28, loss = 1.34967275\n",
      "Iteration 29, loss = 1.34915685\n",
      "Iteration 30, loss = 1.34871293\n",
      "Iteration 31, loss = 1.34865636\n",
      "Iteration 32, loss = 1.34782173\n",
      "Iteration 33, loss = 1.34799764\n",
      "Iteration 34, loss = 1.34749533\n",
      "Iteration 35, loss = 1.34747387\n",
      "Iteration 36, loss = 1.34707925\n",
      "Iteration 37, loss = 1.34701910\n",
      "Iteration 38, loss = 1.34669471\n",
      "Iteration 39, loss = 1.34637387\n",
      "Iteration 40, loss = 1.34622964\n",
      "Iteration 41, loss = 1.34576911\n",
      "Iteration 42, loss = 1.34561575\n",
      "Iteration 43, loss = 1.34543691\n",
      "Iteration 44, loss = 1.34494329\n",
      "Iteration 45, loss = 1.34484796\n",
      "Iteration 46, loss = 1.34442891\n",
      "Iteration 47, loss = 1.34450047\n",
      "Iteration 48, loss = 1.34425555\n",
      "Iteration 49, loss = 1.34391610\n",
      "Iteration 50, loss = 1.34371431\n",
      "Iteration 51, loss = 1.34373249\n",
      "Iteration 52, loss = 1.34320654\n",
      "Iteration 53, loss = 1.34325544\n",
      "Iteration 54, loss = 1.34297260\n",
      "Iteration 55, loss = 1.34286864\n",
      "Iteration 56, loss = 1.34254717\n",
      "Iteration 57, loss = 1.34214021\n",
      "Iteration 58, loss = 1.34238252\n",
      "Iteration 59, loss = 1.34202606\n",
      "Iteration 60, loss = 1.34194018\n",
      "Iteration 61, loss = 1.34165714\n",
      "Iteration 62, loss = 1.34165045\n",
      "Iteration 63, loss = 1.34128502\n",
      "Iteration 64, loss = 1.34113178\n",
      "Iteration 65, loss = 1.34106945\n",
      "Iteration 66, loss = 1.34079202\n",
      "Iteration 67, loss = 1.34061909\n",
      "Iteration 68, loss = 1.34038140\n",
      "Iteration 69, loss = 1.34046859\n",
      "Iteration 70, loss = 1.34008239\n",
      "Iteration 71, loss = 1.33991804\n",
      "Iteration 72, loss = 1.33994966\n",
      "Iteration 73, loss = 1.33973863\n",
      "Iteration 74, loss = 1.33955967\n",
      "Iteration 75, loss = 1.33927927\n",
      "Iteration 76, loss = 1.33915318\n",
      "Iteration 77, loss = 1.33905393\n",
      "Iteration 78, loss = 1.33881603\n",
      "Iteration 79, loss = 1.33875910\n",
      "Iteration 80, loss = 1.33869055\n",
      "Iteration 81, loss = 1.33849121\n",
      "Iteration 82, loss = 1.33841044\n",
      "Iteration 83, loss = 1.33829843\n",
      "Iteration 84, loss = 1.33780279\n",
      "Iteration 85, loss = 1.33802149\n",
      "Iteration 86, loss = 1.33781639\n",
      "Iteration 87, loss = 1.33781684\n",
      "Iteration 88, loss = 1.33736824\n",
      "Iteration 89, loss = 1.33746816\n",
      "Iteration 90, loss = 1.33722624\n",
      "Iteration 91, loss = 1.33701026\n",
      "Iteration 92, loss = 1.33701021\n",
      "Iteration 93, loss = 1.33679836\n",
      "Iteration 94, loss = 1.33670708\n",
      "Iteration 95, loss = 1.33646210\n",
      "Iteration 96, loss = 1.33652772\n",
      "Iteration 97, loss = 1.33630714\n",
      "Iteration 98, loss = 1.33620470\n",
      "Iteration 99, loss = 1.33623657\n",
      "Iteration 100, loss = 1.33598109\n",
      "Iteration 101, loss = 1.33585048\n",
      "Iteration 102, loss = 1.33569540\n",
      "Iteration 103, loss = 1.33551924\n",
      "Iteration 104, loss = 1.33556123\n",
      "Iteration 105, loss = 1.33533158\n",
      "Iteration 106, loss = 1.33514170\n",
      "Iteration 107, loss = 1.33500388\n",
      "Iteration 108, loss = 1.33476624\n",
      "Iteration 109, loss = 1.33489067\n",
      "Iteration 110, loss = 1.33468848\n",
      "Iteration 111, loss = 1.33461397\n",
      "Iteration 112, loss = 1.33423274\n",
      "Iteration 113, loss = 1.33441666\n",
      "Iteration 114, loss = 1.33423589\n",
      "Iteration 115, loss = 1.33388573\n",
      "Iteration 116, loss = 1.33426413\n",
      "Iteration 117, loss = 1.33396289\n",
      "Iteration 118, loss = 1.33379433\n",
      "Iteration 119, loss = 1.33395570\n",
      "Iteration 120, loss = 1.33367703\n",
      "Iteration 121, loss = 1.33348642\n",
      "Iteration 122, loss = 1.33339904\n",
      "Iteration 123, loss = 1.33335125\n",
      "Iteration 124, loss = 1.33330120\n",
      "Iteration 125, loss = 1.33314045\n",
      "Iteration 126, loss = 1.33295453\n",
      "Iteration 127, loss = 1.33279385\n",
      "Iteration 128, loss = 1.33279488\n",
      "Iteration 129, loss = 1.33266766\n",
      "Iteration 130, loss = 1.33263197\n",
      "Iteration 131, loss = 1.33244764\n",
      "Iteration 132, loss = 1.33238185\n",
      "Iteration 133, loss = 1.33233309\n",
      "Iteration 134, loss = 1.33211635\n",
      "Iteration 135, loss = 1.33206129\n",
      "Iteration 136, loss = 1.33191412\n",
      "Iteration 137, loss = 1.33193500\n",
      "Iteration 138, loss = 1.33174531\n",
      "Iteration 139, loss = 1.33164275\n",
      "Iteration 140, loss = 1.33157977\n",
      "Iteration 141, loss = 1.33158093\n",
      "Iteration 142, loss = 1.33150074\n",
      "Iteration 143, loss = 1.33117412\n",
      "Iteration 144, loss = 1.33116357\n",
      "Iteration 145, loss = 1.33111280\n",
      "Iteration 146, loss = 1.33101557\n",
      "Iteration 147, loss = 1.33097566\n",
      "Iteration 148, loss = 1.33084151\n",
      "Iteration 149, loss = 1.33060484\n",
      "Iteration 150, loss = 1.33070459\n",
      "Iteration 151, loss = 1.33060298\n",
      "Iteration 152, loss = 1.33041688\n",
      "Iteration 153, loss = 1.33034815\n",
      "Iteration 154, loss = 1.33019880\n",
      "Iteration 155, loss = 1.33029594\n",
      "Iteration 156, loss = 1.33010516\n",
      "Iteration 157, loss = 1.32992872\n",
      "Iteration 158, loss = 1.32984731\n",
      "Iteration 159, loss = 1.32972096\n",
      "Iteration 160, loss = 1.32968075\n",
      "Iteration 161, loss = 1.32959529\n",
      "Iteration 162, loss = 1.32949004\n",
      "Iteration 163, loss = 1.32923991\n",
      "Iteration 164, loss = 1.32951352\n",
      "Iteration 165, loss = 1.32934075\n",
      "Iteration 166, loss = 1.32920078\n",
      "Iteration 167, loss = 1.32911943\n",
      "Iteration 168, loss = 1.32898543\n",
      "Iteration 169, loss = 1.32894605\n",
      "Iteration 170, loss = 1.32869095\n",
      "Iteration 171, loss = 1.32877422\n",
      "Iteration 172, loss = 1.32867062\n",
      "Iteration 173, loss = 1.32861083\n",
      "Iteration 174, loss = 1.32855360\n",
      "Iteration 175, loss = 1.32839278\n",
      "Iteration 176, loss = 1.32834345\n",
      "Iteration 177, loss = 1.32824340\n",
      "Iteration 178, loss = 1.32808126\n",
      "Iteration 179, loss = 1.32813627\n",
      "Iteration 180, loss = 1.32815307\n",
      "Iteration 181, loss = 1.32796064\n",
      "Iteration 182, loss = 1.32788895\n",
      "Iteration 183, loss = 1.32779912\n",
      "Iteration 184, loss = 1.32772729\n",
      "Iteration 185, loss = 1.32754699\n",
      "Iteration 186, loss = 1.32747568\n",
      "Iteration 187, loss = 1.32727646\n",
      "Iteration 188, loss = 1.32744426\n",
      "Iteration 189, loss = 1.32735706\n",
      "Iteration 190, loss = 1.32732263\n",
      "Iteration 191, loss = 1.32716804\n",
      "Iteration 192, loss = 1.32695542\n",
      "Iteration 193, loss = 1.32709863\n",
      "Iteration 194, loss = 1.32680616\n",
      "Iteration 195, loss = 1.32680788\n",
      "Iteration 196, loss = 1.32691740\n",
      "Iteration 197, loss = 1.32686565\n",
      "Iteration 198, loss = 1.32667633\n",
      "Iteration 199, loss = 1.32653160\n",
      "Iteration 200, loss = 1.32654795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.5642\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      1985\n",
      "           1       0.67      0.70      0.68      1889\n",
      "           2       0.42      0.59      0.49      1384\n",
      "           3       0.14      0.54      0.23       531\n",
      "           4       0.71      0.71      0.71      2067\n",
      "\n",
      "   micro avg       0.56      0.72      0.63      7856\n",
      "   macro avg       0.56      0.68      0.60      7856\n",
      "weighted avg       0.65      0.72      0.68      7856\n",
      " samples avg       0.56      0.56      0.56      7856\n",
      "\n",
      "accuracy on test data:  0.562\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       225\n",
      "           1       0.64      0.68      0.66       184\n",
      "           2       0.40      0.61      0.48       130\n",
      "           3       0.18      0.54      0.27        63\n",
      "           4       0.63      0.68      0.66       173\n",
      "\n",
      "   micro avg       0.56      0.73      0.63       775\n",
      "   macro avg       0.55      0.69      0.59       775\n",
      "weighted avg       0.63      0.73      0.67       775\n",
      " samples avg       0.56      0.56      0.56       775\n",
      "\n",
      "\n",
      "\n",
      "Hidden layer: [512, 256, 128, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81368436\n",
      "Iteration 2, loss = 1.44544842\n",
      "Iteration 3, loss = 1.38917601\n",
      "Iteration 4, loss = 1.38138721\n",
      "Iteration 5, loss = 1.37689060\n",
      "Iteration 6, loss = 1.37331294\n",
      "Iteration 7, loss = 1.37047256\n",
      "Iteration 8, loss = 1.36867094\n",
      "Iteration 9, loss = 1.36701830\n",
      "Iteration 10, loss = 1.36629762\n",
      "Iteration 11, loss = 1.36428958\n",
      "Iteration 12, loss = 1.36411254\n",
      "Iteration 13, loss = 1.36277406\n",
      "Iteration 14, loss = 1.36201276\n",
      "Iteration 15, loss = 1.36122268\n",
      "Iteration 16, loss = 1.36026491\n",
      "Iteration 17, loss = 1.35992904\n",
      "Iteration 18, loss = 1.35911954\n",
      "Iteration 19, loss = 1.35855620\n",
      "Iteration 20, loss = 1.35809020\n",
      "Iteration 21, loss = 1.35724215\n",
      "Iteration 22, loss = 1.35677803\n",
      "Iteration 23, loss = 1.35652282\n",
      "Iteration 24, loss = 1.35581300\n",
      "Iteration 25, loss = 1.35539375\n",
      "Iteration 26, loss = 1.35481140\n",
      "Iteration 27, loss = 1.35403886\n",
      "Iteration 28, loss = 1.35396309\n",
      "Iteration 29, loss = 1.35331822\n",
      "Iteration 30, loss = 1.35319095\n",
      "Iteration 31, loss = 1.35287776\n",
      "Iteration 32, loss = 1.35243008\n",
      "Iteration 33, loss = 1.35148415\n",
      "Iteration 34, loss = 1.35169121\n",
      "Iteration 35, loss = 1.35138144\n",
      "Iteration 36, loss = 1.35108106\n",
      "Iteration 37, loss = 1.35077123\n",
      "Iteration 38, loss = 1.35049022\n",
      "Iteration 39, loss = 1.35019795\n",
      "Iteration 40, loss = 1.34961928\n",
      "Iteration 41, loss = 1.34928929\n",
      "Iteration 42, loss = 1.34920142\n",
      "Iteration 43, loss = 1.34881146\n",
      "Iteration 44, loss = 1.34820406\n",
      "Iteration 45, loss = 1.34864597\n",
      "Iteration 46, loss = 1.34793955\n",
      "Iteration 47, loss = 1.34757309\n",
      "Iteration 48, loss = 1.34729816\n",
      "Iteration 49, loss = 1.34697372\n",
      "Iteration 50, loss = 1.34694880\n",
      "Iteration 51, loss = 1.34656289\n",
      "Iteration 52, loss = 1.34625051\n",
      "Iteration 53, loss = 1.34602164\n",
      "Iteration 54, loss = 1.34617614\n",
      "Iteration 55, loss = 1.34566605\n",
      "Iteration 56, loss = 1.34559899\n",
      "Iteration 57, loss = 1.34530104\n",
      "Iteration 58, loss = 1.34506928\n",
      "Iteration 59, loss = 1.34490077\n",
      "Iteration 60, loss = 1.34454417\n",
      "Iteration 61, loss = 1.34442064\n",
      "Iteration 62, loss = 1.34423867\n",
      "Iteration 63, loss = 1.34389345\n",
      "Iteration 64, loss = 1.34411251\n",
      "Iteration 65, loss = 1.34349359\n",
      "Iteration 66, loss = 1.34326719\n",
      "Iteration 67, loss = 1.34327519\n",
      "Iteration 68, loss = 1.34291595\n",
      "Iteration 69, loss = 1.34269664\n",
      "Iteration 70, loss = 1.34257381\n",
      "Iteration 71, loss = 1.34238822\n",
      "Iteration 72, loss = 1.34225701\n",
      "Iteration 73, loss = 1.34217715\n",
      "Iteration 74, loss = 1.34199308\n",
      "Iteration 75, loss = 1.34156418\n",
      "Iteration 76, loss = 1.34145023\n",
      "Iteration 77, loss = 1.34130128\n",
      "Iteration 78, loss = 1.34107888\n",
      "Iteration 79, loss = 1.34097266\n",
      "Iteration 80, loss = 1.34051020\n",
      "Iteration 81, loss = 1.34031797\n",
      "Iteration 82, loss = 1.34050280\n",
      "Iteration 83, loss = 1.34022977\n",
      "Iteration 84, loss = 1.34014982\n",
      "Iteration 85, loss = 1.33990893\n",
      "Iteration 86, loss = 1.33985747\n",
      "Iteration 87, loss = 1.33962806\n",
      "Iteration 88, loss = 1.33948541\n",
      "Iteration 89, loss = 1.33930273\n",
      "Iteration 90, loss = 1.33923741\n",
      "Iteration 91, loss = 1.33903170\n",
      "Iteration 92, loss = 1.33890641\n",
      "Iteration 93, loss = 1.33867310\n",
      "Iteration 94, loss = 1.33863891\n",
      "Iteration 95, loss = 1.33824019\n",
      "Iteration 96, loss = 1.33817394\n",
      "Iteration 97, loss = 1.33787108\n",
      "Iteration 98, loss = 1.33795980\n",
      "Iteration 99, loss = 1.33778427\n",
      "Iteration 100, loss = 1.33762673\n",
      "Iteration 101, loss = 1.33748697\n",
      "Iteration 102, loss = 1.33726676\n",
      "Iteration 103, loss = 1.33727729\n",
      "Iteration 104, loss = 1.33722797\n",
      "Iteration 105, loss = 1.33697452\n",
      "Iteration 106, loss = 1.33682430\n",
      "Iteration 107, loss = 1.33675692\n",
      "Iteration 108, loss = 1.33645577\n",
      "Iteration 109, loss = 1.33642302\n",
      "Iteration 110, loss = 1.33632492\n",
      "Iteration 111, loss = 1.33604307\n",
      "Iteration 112, loss = 1.33606265\n",
      "Iteration 113, loss = 1.33586589\n",
      "Iteration 114, loss = 1.33588622\n",
      "Iteration 115, loss = 1.33560642\n",
      "Iteration 116, loss = 1.33555894\n",
      "Iteration 117, loss = 1.33544376\n",
      "Iteration 118, loss = 1.33528809\n",
      "Iteration 119, loss = 1.33515806\n",
      "Iteration 120, loss = 1.33508599\n",
      "Iteration 121, loss = 1.33489823\n",
      "Iteration 122, loss = 1.33480901\n",
      "Iteration 123, loss = 1.33486307\n",
      "Iteration 124, loss = 1.33455791\n",
      "Iteration 125, loss = 1.33450993\n",
      "Iteration 126, loss = 1.33431833\n",
      "Iteration 127, loss = 1.33432848\n",
      "Iteration 128, loss = 1.33407660\n",
      "Iteration 129, loss = 1.33384337\n",
      "Iteration 130, loss = 1.33399172\n",
      "Iteration 131, loss = 1.33385179\n",
      "Iteration 132, loss = 1.33338531\n",
      "Iteration 133, loss = 1.33348674\n",
      "Iteration 134, loss = 1.33337484\n",
      "Iteration 135, loss = 1.33334390\n",
      "Iteration 136, loss = 1.33319013\n",
      "Iteration 137, loss = 1.33311663\n",
      "Iteration 138, loss = 1.33289270\n",
      "Iteration 139, loss = 1.33284942\n",
      "Iteration 140, loss = 1.33269074\n",
      "Iteration 141, loss = 1.33260482\n",
      "Iteration 142, loss = 1.33249308\n",
      "Iteration 143, loss = 1.33238653\n",
      "Iteration 144, loss = 1.33241869\n",
      "Iteration 145, loss = 1.33235419\n",
      "Iteration 146, loss = 1.33195410\n",
      "Iteration 147, loss = 1.33200446\n",
      "Iteration 148, loss = 1.33184600\n",
      "Iteration 149, loss = 1.33173865\n",
      "Iteration 150, loss = 1.33163852\n",
      "Iteration 151, loss = 1.33162311\n",
      "Iteration 152, loss = 1.33157170\n",
      "Iteration 153, loss = 1.33124986\n",
      "Iteration 154, loss = 1.33114563\n",
      "Iteration 155, loss = 1.33117362\n",
      "Iteration 156, loss = 1.33111677\n",
      "Iteration 157, loss = 1.33091936\n",
      "Iteration 158, loss = 1.33059599\n",
      "Iteration 159, loss = 1.33069087\n",
      "Iteration 160, loss = 1.33072471\n",
      "Iteration 161, loss = 1.33060597\n",
      "Iteration 162, loss = 1.33048843\n",
      "Iteration 163, loss = 1.33034758\n",
      "Iteration 164, loss = 1.33026873\n",
      "Iteration 165, loss = 1.32985650\n",
      "Iteration 166, loss = 1.33006810\n",
      "Iteration 167, loss = 1.33002381\n",
      "Iteration 168, loss = 1.33001853\n",
      "Iteration 169, loss = 1.32981868\n",
      "Iteration 170, loss = 1.32967690\n",
      "Iteration 171, loss = 1.32967191\n",
      "Iteration 172, loss = 1.32964922\n",
      "Iteration 173, loss = 1.32915260\n",
      "Iteration 174, loss = 1.32933370\n",
      "Iteration 175, loss = 1.32919457\n",
      "Iteration 176, loss = 1.32925131\n",
      "Iteration 177, loss = 1.32907655\n",
      "Iteration 178, loss = 1.32889414\n",
      "Iteration 179, loss = 1.32900540\n",
      "Iteration 180, loss = 1.32868908\n",
      "Iteration 181, loss = 1.32884859\n",
      "Iteration 182, loss = 1.32870995\n",
      "Iteration 183, loss = 1.32838215\n",
      "Iteration 184, loss = 1.32848429\n",
      "Iteration 185, loss = 1.32849773\n",
      "Iteration 186, loss = 1.32830025\n",
      "Iteration 187, loss = 1.32789618\n",
      "Iteration 188, loss = 1.32815089\n",
      "Iteration 189, loss = 1.32800030\n",
      "Iteration 190, loss = 1.32774316\n",
      "Iteration 191, loss = 1.32808793\n",
      "Iteration 192, loss = 1.32764538\n",
      "Iteration 193, loss = 1.32781256\n",
      "Iteration 194, loss = 1.32763628\n",
      "Iteration 195, loss = 1.32751946\n",
      "Iteration 196, loss = 1.32739331\n",
      "Iteration 197, loss = 1.32729238\n",
      "Iteration 198, loss = 1.32733673\n",
      "Iteration 199, loss = 1.32719312\n",
      "Iteration 200, loss = 1.32718437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train data:  0.5846\n",
      "metrics for train data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      1997\n",
      "           1       0.68      0.69      0.69      1958\n",
      "           2       0.46      0.58      0.51      1568\n",
      "           3       0.18      0.53      0.27       688\n",
      "           4       0.71      0.71      0.71      2064\n",
      "\n",
      "   micro avg       0.58      0.71      0.64      8275\n",
      "   macro avg       0.58      0.68      0.61      8275\n",
      "weighted avg       0.66      0.71      0.67      8275\n",
      " samples avg       0.58      0.58      0.58      8275\n",
      "\n",
      "accuracy on test data:  0.581\n",
      "metrics for test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       228\n",
      "           1       0.66      0.68      0.67       194\n",
      "           2       0.44      0.59      0.51       149\n",
      "           3       0.20      0.45      0.28        85\n",
      "           4       0.64      0.69      0.66       172\n",
      "\n",
      "   micro avg       0.58      0.70      0.64       828\n",
      "   macro avg       0.57      0.66      0.60       828\n",
      "weighted avg       0.64      0.70      0.66       828\n",
      " samples avg       0.58      0.58      0.58       828\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tkarthikeyan/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [[512], [512,256], [512,256,128], [512,256,128,64]]\n",
    "network_depth = [1,2,3,4]\n",
    "f1_score_train = []\n",
    "f1_score_test = []\n",
    "for hidden_layer in hidden_layers:\n",
    "    print(f\"Hidden layer: {hidden_layer}\")\n",
    "    clf = MLPClassifier(activation=\"relu\", solver=\"sgd\", alpha = 0, batch_size=32, hidden_layer_sizes=np.array(hidden_layer), learning_rate=\"invscaling\", tol=5e-6, n_iter_no_change=5, verbose=True, learning_rate_init=0.01).fit(X_train, y_train_onehot)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy on train data: \",accuracy_score(y_train_onehot, y_pred_train))\n",
    "    print(\"metrics for train data: \")\n",
    "    get_metric(y_train_onehot, y_pred_train)\n",
    "    f1_score_train.append(f1_score(y_train_onehot, y_pred_train, average=\"macro\"))\n",
    "    \n",
    "    print(\"accuracy on test data: \",accuracy_score(y_test_onehot, y_pred_test))\n",
    "    print(\"metrics for test data: \")\n",
    "    get_metric(y_test_onehot, y_pred_test)\n",
    "    f1_score_test.append(f1_score(y_test_onehot, y_pred_test, average=\"macro\"))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot avg f1_scores for different depth\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(network_depth, f1_score_train, marker='o', markersize=6, color='blue', label='Avg f1_score train data')\n",
    "plt.plot(network_depth, f1_score_test, marker='o', markersize=6, color='red', label='Avg f1_score test data')\n",
    "\n",
    "plt.title('Avg f1_score vs network depth')\n",
    "plt.xlabel('network depth')\n",
    "plt.ylabel('Avg f1_score')\n",
    "plt.xticks(network_depth)\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
